{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1zVoaXE-pNjU5ZMJ8sfs276f7KA5sETEP","timestamp":1679499930218},{"file_id":"16T-T3ZLfP0Y11ylD44XLyP_sNBSYr--u","timestamp":1679136776498},{"file_id":"1EcLWhzpbwW9RvWxGBWiktlRxZ9Bpubdt","timestamp":1679131151492},{"file_id":"1MdY4u6Np8H_OHQ7_8ZysZPrLDiT5u3Wp","timestamp":1678120578286},{"file_id":"1CGFs52_r3H61DdegXY5lXMYboZx0Twj_","timestamp":1678022484099},{"file_id":"11WNWw9_cTpHYNSNYxEkEqvahEbkhQ_up","timestamp":1676981549637}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"1b3940d67db54f7d8dfc92de16334916":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ab0a6341efc04c0b90f73925b2f7604f","IPY_MODEL_cce9cf9fe5004f4da42c387e358e4bd4","IPY_MODEL_447eb309c1a2495fb4c4eceff3bb8773"],"layout":"IPY_MODEL_fba97a43877540a79c546543178d52e6"}},"ab0a6341efc04c0b90f73925b2f7604f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8a1457cc3c7a47baaad073bbe113880b","placeholder":"​","style":"IPY_MODEL_a2ff46e5a7654dec8fbc2a9d6f9e7ad5","value":"Downloading (…)lve/main/config.json: 100%"}},"cce9cf9fe5004f4da42c387e358e4bd4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c85b1aa55f59400091bd62bfc8e8339f","max":694,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9b78af5842d4487bb98e6d6cb4667248","value":694}},"447eb309c1a2495fb4c4eceff3bb8773":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_37db87fd979c420eae00e5023ef0dd3b","placeholder":"​","style":"IPY_MODEL_b58bb18ef6154ece9aee5047589464c4","value":" 694/694 [00:00&lt;00:00, 37.4kB/s]"}},"fba97a43877540a79c546543178d52e6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a1457cc3c7a47baaad073bbe113880b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a2ff46e5a7654dec8fbc2a9d6f9e7ad5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c85b1aa55f59400091bd62bfc8e8339f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9b78af5842d4487bb98e6d6cb4667248":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"37db87fd979c420eae00e5023ef0dd3b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b58bb18ef6154ece9aee5047589464c4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"642bebccb3ed4c3f9ebd026233b5f347":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d982186bc96f48a5a9e4c8e3424260c1","IPY_MODEL_5ababc13bb0e47c181884c260c4244f4","IPY_MODEL_3e9c86e1e5af42888643067bac6eefcc"],"layout":"IPY_MODEL_22675d723fdd4a67a43f2043b35cda7b"}},"d982186bc96f48a5a9e4c8e3424260c1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aafb86e2f7424728afd98845c3dfd2dd","placeholder":"​","style":"IPY_MODEL_ffc33a37391a41db8b675be1bcca9d71","value":"Downloading (…)olve/main/vocab.json: 100%"}},"5ababc13bb0e47c181884c260c4244f4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2da7461a95ce4dc69bf41a8096495a6d","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_df4820a5221346f1923be5d30adf5075","value":898823}},"3e9c86e1e5af42888643067bac6eefcc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7a3e7dfdca54b1c923a255e3323663c","placeholder":"​","style":"IPY_MODEL_e7416cd1af2246619cdcdfd531750b5b","value":" 899k/899k [00:00&lt;00:00, 2.09MB/s]"}},"22675d723fdd4a67a43f2043b35cda7b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aafb86e2f7424728afd98845c3dfd2dd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ffc33a37391a41db8b675be1bcca9d71":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2da7461a95ce4dc69bf41a8096495a6d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"df4820a5221346f1923be5d30adf5075":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f7a3e7dfdca54b1c923a255e3323663c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7416cd1af2246619cdcdfd531750b5b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"19e09c682df542d6bbcea52c83e9c125":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_653022c0abff4dadbba45b1090fa4aea","IPY_MODEL_47ee2ad342f64039a877372f82a5f43e","IPY_MODEL_574455d57c4847359d8ffd563ed78b62"],"layout":"IPY_MODEL_2232b644e38b46519b5b53af8bc51de0"}},"653022c0abff4dadbba45b1090fa4aea":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ac669a74e7e46fda4925bbbf08c3ddf","placeholder":"​","style":"IPY_MODEL_18020a2b04664381ae0a9a6eed4a09db","value":"Downloading (…)olve/main/merges.txt: 100%"}},"47ee2ad342f64039a877372f82a5f43e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8953c0170d0b41e9997f23541f588ed5","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8fade4918a9648aab65543c2e4ae8002","value":456318}},"574455d57c4847359d8ffd563ed78b62":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1f436c2d235043cb809037c998f56ccb","placeholder":"​","style":"IPY_MODEL_86010862c6804b649928bdb1977caf01","value":" 456k/456k [00:00&lt;00:00, 1.27MB/s]"}},"2232b644e38b46519b5b53af8bc51de0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ac669a74e7e46fda4925bbbf08c3ddf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"18020a2b04664381ae0a9a6eed4a09db":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8953c0170d0b41e9997f23541f588ed5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8fade4918a9648aab65543c2e4ae8002":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1f436c2d235043cb809037c998f56ccb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86010862c6804b649928bdb1977caf01":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c19c808accb44a90a4d56f046ca1dfdd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f96620f48af9455c8610b90866034af9","IPY_MODEL_7700d216fc6b4217bfef4f6f0c663d31","IPY_MODEL_be147dd1695549868ce4e6eb037ed000"],"layout":"IPY_MODEL_c3bb1a4f986e41a3b7787610dbb23897"}},"f96620f48af9455c8610b90866034af9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_51e94cef2ea54fb1a1c77d8470ec4bb7","placeholder":"​","style":"IPY_MODEL_46d4509f679a48f382498ea8281eac3e","value":"Downloading (…)/main/tokenizer.json: 100%"}},"7700d216fc6b4217bfef4f6f0c663d31":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6bef93a1941d420e8223717128c96b79","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f9e0f0547f214ed48409d2c77c71f8c4","value":1355863}},"be147dd1695549868ce4e6eb037ed000":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_df1af0bae30f482abc3315785297fe68","placeholder":"​","style":"IPY_MODEL_1720daab0b57419f8729dd74519d96b9","value":" 1.36M/1.36M [00:00&lt;00:00, 2.65MB/s]"}},"c3bb1a4f986e41a3b7787610dbb23897":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51e94cef2ea54fb1a1c77d8470ec4bb7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"46d4509f679a48f382498ea8281eac3e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6bef93a1941d420e8223717128c96b79":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9e0f0547f214ed48409d2c77c71f8c4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"df1af0bae30f482abc3315785297fe68":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1720daab0b57419f8729dd74519d96b9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d09d002581cf479e976907f9ff7d8205":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fa0b993c3f28411fa482ae1ad932b1f3","IPY_MODEL_cc8ec622085448d682d93c6834b29785","IPY_MODEL_3115a628b95c48d58cabf6718ae66c5d"],"layout":"IPY_MODEL_8adb63a6c852441d9e48cd3ab6c9b154"}},"fa0b993c3f28411fa482ae1ad932b1f3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f83d76332898468db2ff7c5f54609c61","placeholder":"​","style":"IPY_MODEL_2df7162dd48443d4b8375a0b7bf1ad1e","value":"Downloading builder script: "}},"cc8ec622085448d682d93c6834b29785":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e622836f77a04b3cad68768b363e9ccd","max":1652,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2a806d80770e48079e99927953aa2725","value":1652}},"3115a628b95c48d58cabf6718ae66c5d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1af82d14b9bc452991d46f443911fa47","placeholder":"​","style":"IPY_MODEL_3e8bea9e21984858bd906126ede010b3","value":" 4.21k/? [00:00&lt;00:00, 339kB/s]"}},"8adb63a6c852441d9e48cd3ab6c9b154":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f83d76332898468db2ff7c5f54609c61":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2df7162dd48443d4b8375a0b7bf1ad1e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e622836f77a04b3cad68768b363e9ccd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a806d80770e48079e99927953aa2725":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1af82d14b9bc452991d46f443911fa47":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e8bea9e21984858bd906126ede010b3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3a270831382f4f288b99fb408068570c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b412b901863b45feb83bb8838c78eeec","IPY_MODEL_bec33c01091541b8b6a995e5e798fee1","IPY_MODEL_61bad095219d4b43bf523044ef3d881c"],"layout":"IPY_MODEL_1f878c3ec7534f58a4e2939de16a3394"}},"b412b901863b45feb83bb8838c78eeec":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a1e2514e061e43389a71797b3bfd5979","placeholder":"​","style":"IPY_MODEL_40ade0a89cf34d85a48deaeb7479aacf","value":"Downloading pytorch_model.bin: 100%"}},"bec33c01091541b8b6a995e5e798fee1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0fd437c2d4b94a5f82eb5ffc2e3b53ed","max":597257159,"min":0,"orientation":"horizontal","style":"IPY_MODEL_79b54843a9644940b4c84b7ab3330325","value":597257159}},"61bad095219d4b43bf523044ef3d881c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d8112dd59deb4569bd6357975886f488","placeholder":"​","style":"IPY_MODEL_1a5533a7b2614321b9cd380ba8b5855f","value":" 597M/597M [00:02&lt;00:00, 318MB/s]"}},"1f878c3ec7534f58a4e2939de16a3394":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1e2514e061e43389a71797b3bfd5979":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40ade0a89cf34d85a48deaeb7479aacf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0fd437c2d4b94a5f82eb5ffc2e3b53ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"79b54843a9644940b4c84b7ab3330325":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d8112dd59deb4569bd6357975886f488":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a5533a7b2614321b9cd380ba8b5855f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d850d82a9b494c1596e9a39b49a539c3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_96408953079b4b12ad2ce2641fe26327","IPY_MODEL_1e2ee61c1165475ebe4a1506eb0d4672","IPY_MODEL_3785385707704df99b34da0e82a8ff2a"],"layout":"IPY_MODEL_349c223e7a4d4222971f1f7d610f97d5"}},"96408953079b4b12ad2ce2641fe26327":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4e5251da32c44018ae40b706b6df63e9","placeholder":"​","style":"IPY_MODEL_8b68421853494acdb31d3f48d741a991","value":"  0%"}},"1e2ee61c1165475ebe4a1506eb0d4672":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_72dbcea5dfcb425f9e9e15c4c667f9b7","max":1305,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a7f729c185f149be95ae2eeea6cf2362","value":0}},"3785385707704df99b34da0e82a8ff2a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9cb07cac73184dc49a461a6b5cbe20d1","placeholder":"​","style":"IPY_MODEL_8bf7af91b3344e6da4cae2031d590263","value":" 0/1305 [00:00&lt;?, ?it/s]"}},"349c223e7a4d4222971f1f7d610f97d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e5251da32c44018ae40b706b6df63e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b68421853494acdb31d3f48d741a991":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"72dbcea5dfcb425f9e9e15c4c667f9b7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7f729c185f149be95ae2eeea6cf2362":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9cb07cac73184dc49a461a6b5cbe20d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8bf7af91b3344e6da4cae2031d590263":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU","gpuClass":"premium"},"cells":[{"cell_type":"markdown","source":["# Importación de librerías y parámetros"],"metadata":{"id":"H7uHO0xNibAs"}},{"cell_type":"code","source":["base_path = '/drive/My Drive/CorpusPFG/'\n","\n","#Datasets procesados\n","dataset_path = base_path + 'Dataset'\n","\n","#Model parameters\n","#CHECKPOINT = \"allenai/led-base-16384\"\n","CHECKPOINT = \"allenai/longformer-base-4096\"\n","#CHECKPOINT = \"bert-base-uncased\"\n","MODEL_OUTPUT_DIR = \"uned-tfg-07.02\"\n","NUM_EPOCHS = 5\n","BATCH_SIZE = 4\n","METRIC_NAME = \"f1\"\n","HUGGING_FACE_TOKEN = \"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\""],"metadata":{"id":"rhtVG5QtiYkr","executionInfo":{"status":"ok","timestamp":1679507209126,"user_tz":0,"elapsed":332,"user":{"displayName":"Alexis Domínguez","userId":"12545493388305103028"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["## Set-up environment\n","\n","First, we install the libraries which we'll use: HuggingFace Transformers and Datasets."],"metadata":{"id":"cnxpm2C6Fqxt"}},{"cell_type":"code","source":["!pip install -q datasets evaluate transformers[sentencepiece] accelerate\n","# To run the training on TPU, you will need to uncomment the following line:\n","#!pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl"],"metadata":{"id":"2LNboFxnxK--","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679507229027,"user_tz":0,"elapsed":18999,"user":{"displayName":"Alexis Domínguez","userId":"12545493388305103028"}},"outputId":"fbbf0255-9631-4a75-f1a1-08ea80204012"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 KB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 KB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m105.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.8/212.8 KB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 KB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 KB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 KB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m101.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"markdown","source":["## Load dataset\n","\n","Next, let's load a multi-label text classification dataset from files.\n"],"metadata":{"id":"UujvKjusiVfn"}},{"cell_type":"code","source":["from google.colab import drive\n","\n","drive.mount('/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UisW21-kiQqd","executionInfo":{"status":"ok","timestamp":1679507249364,"user_tz":0,"elapsed":20347,"user":{"displayName":"Alexis Domínguez","userId":"12545493388305103028"}},"outputId":"af7dfc29-d1b6-4d40-e9c5-e5cc317d31a5"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /drive\n"]}]},{"cell_type":"code","source":["from datasets import DatasetDict\n","\n","dataset = DatasetDict.load_from_disk(dataset_path)"],"metadata":{"id":"knWPWK7GAHoZ","executionInfo":{"status":"ok","timestamp":1679507256220,"user_tz":0,"elapsed":6858,"user":{"displayName":"Alexis Domínguez","userId":"12545493388305103028"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UzxNkhLMCfOX","executionInfo":{"status":"ok","timestamp":1679507256220,"user_tz":0,"elapsed":14,"user":{"displayName":"Alexis Domínguez","userId":"12545493388305103028"}},"outputId":"c4327ad2-5a07-4505-a976-8f457446ead8"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['acto', 'label', 'label_str', 'labels_str', 'informes', 'text'],\n","        num_rows: 2088\n","    })\n","    validation: Dataset({\n","        features: ['acto', 'label', 'label_str', 'labels_str', 'informes', 'text'],\n","        num_rows: 233\n","    })\n","    test: Dataset({\n","        features: ['acto', 'label', 'label_str', 'labels_str', 'informes', 'text'],\n","        num_rows: 583\n","    })\n","})"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["Let's check the an example of the training split:"],"metadata":{"id":"NAQXoTKZCo52"}},{"cell_type":"code","source":["dataset['train'][0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FFcoXzf_Cifb","executionInfo":{"status":"ok","timestamp":1679507256221,"user_tz":0,"elapsed":13,"user":{"displayName":"Alexis Domínguez","userId":"12545493388305103028"}},"outputId":"c1d47db5-9cc0-4d9f-f51b-8ed76a54dfed"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'acto': 26792043,\n"," 'label': 256,\n"," 'label_str': 'T44.7X5A',\n"," 'labels_str': ['T44.7X5A', 'I95.2'],\n"," 'informes': ['26792043-169143208.txt'],\n"," 'text': ' _CABECERA_XXXX_  \\nSexo :Mujer\\n\\tINFORME ALTA DE MEDICINA INTERNA\\n\\tNIF/ _DATA_XXXX_ \\n\\t _DATA_XXXX_ \\n\\t _DATA_XXXX_ \\n\\t _DATA_XXXX_ \\n\\t_NOMBRE_XXXX_  \\n\\tTeléfono:  _DATA_XXXX_ \\n\\tTeléfono Móvil:  _DATA_XXXX_ \\n\\tFecha ingreso:  _FECHA_XXXX_   _HORA_XXXX_ \\nFecha alta:        _FECHA_XXXX_ \\n\\t _DOC_XXXX_ \\n\\tTipo de Ingreso: Desde urgencias\\n\\tMotivo de Ingreso (GP) Síncope posible hipotensivo. Problema social\\nMotivo de Alta\\n _APELLIDO_XXXX_ \\nMujer de 88 años.\\nAntecedentes Personales\\nNo refiere alergias conocidas a medicamentos\\nHTA. Cardiopatía hipertensiva con HVI moderada valorada por cardiología en 2008. Estudio de isquemia MIBI-DIPI negativo.\\nDM-2 sin tratamiento farmacológico.\\nDL.\\nNeoplasia maligna de útero \\nCarcinoma basocelular.\\nHerpes zoster torácico en 2010.\\nLumbalgia crónica mecánica. Aplastamiento de L1.\\nSd depresivo.\\nDeterioro cognitivo leve. Valorada por Neurología.\\n*Ingreso en 2015 en MINT por Neumonía LSD e ITU. Presentó SCA durante el ingreso.\\n*Episodios de urgencias por episodios de desorientación.\\nANTECEDENTES QUIRÚRGICOS: Cataratas bilateral, 2 cesáreas, fisura anal hace 15 años. Absceso isquiorectal (2008), reintervenido en dos ocasiones por proceso fistuloso.\\nSITUACIÓN BASAL: Vive en domicilio sola, activa e independiente para las ABVD. Doble continente. Deterioro cognitivo leve. No disnea ni ortopnea. No edemas en MMII. Anda con bastón.\\nTRATAMIENTO HABITUAL: COROPRES 25 1-0-0. DONEPEZILO 10. METAMIZOL. OMEPRAZOL 20 1-0-0. SERTRALINA 5 1-0-0,5. TRAMADOL/PARACETAMOL.\\nEnfermedad Actual\\nMujer de 88 años que acude a urgencias tras ser encontrada por su hijo en el suelo a las 19.00. La paciente presentó síncope con amnesia  aunque afirma pródromos al síncope consistente en visión borrosa y sudoración y en relación con la adopción de la bipedestación. Múltiples caídas de  _FECHA_XXXX_  para aquí en el contexto de la bipedestación, estando en el mercado. Asegura que siempre se cae hacia atrás con TCE occipital. Nota que las piernas no le aguantan y claudican. No relajación de esfínteres ni mordedura de lengua. Escoriaciones en ambos codos, y hematomas en ambas rodillas, dolor costal y lumbar. No dolor torácico previo ni palpitaciones ni sd miccional, ni dolor abdominal ni diarrea.\\nExploración Física\\nUrgencias\\nTA: 157/66; FC: 52 lpm. 95% basal.\\nHematomas en brazo izquierdo y pierna izquierda antiguos y recientes. A la exploración de hombro izquierdo no existe patrón capsular ni evidencia de rotura. Dolor a la palpación, no dolor a la movilización pasiva del miembro. AP: MVC sin ruidos sobreañadidos. AC: Rítmica sin soplos. ABD: RHA+, dolor a la palpación difusa, no masas ni megalias, timpánica, no peritonismo. MMII: No edemas, pulsos pedios conservados. NRL: No focalidad neurológica, moviliza 4 miembros, no rigidez de nuca, pares craneales normales, funciones superiores conservadas.\\nExploraciones Complementarias\\n  _FECHA_XXXX_  ECG: P sinusal  51 lpm. Rítmico. Eje normal. PR normal . QRS estrecho. QTc. No bloqueos ni datos de crecimiento de cavidades. Sin alteraciones agudas de la repolarización.\\n  _FECHA_XXXX_  Rx Tórax: No se observan claros infiltrados ni consolidaciones. No derrame pleural. ICT en límites de la normalidad.\\n  _FECHA_XXXX_  TC  craneal: No  se  observan  signos  de  sangrado  reciente,  colecciones  ni  efecto  de  masa  intra  ni  extraaxial. Correcta  diferenciación  entre  sustancia  blanca  y  sustancia  gris  supra  e  infratentorialmente,  apreciando  lesiones  hipodensas  en  sustancia  blanca  periventricular  en  probable  relación  con  enfermedad  isquémica  de  pequeño  vaso. Prominencia  de  surcos  y  ventrículos  en  relación  con  atrofia  corticosubcortical  difusa. Cisternas  libres.  Línea  media  centrada.  Ateromatosis  calcificada  de  sifones  carotídeos  y  arterias  vertebrales. Hiperostosis  frontal  interna \\nConclusión: \\nNo  se  observan  cambios  significativos  con  respecto  a  TC  previa  del  19/01/2015.\\n _FECHA_XXXX_  – Hemograma: LEU: 9.29 10^3/µL (3.50-11.00); Neut: 6.65 10^3/µL (2.0-7.5); linfoc: 1.77 10^3/µL (1.0-4.5); Monoci: 0.60 10^3/µL (0.2-0.8); Eosino: 0.00 10^3/µL (0.0-0.5); Basófi: 0.05 10^3/µL (0.0-0.2); LUC: 0.22 10^3µL (< =0); %Neut: 71.60 % (40.0-75.0); %Linfo: 19.10 % (20.0-45.0); %Monoc: 6.40 % (2.0-10.0); %Eosin: 0.00 % (1.0-6.0); %Basóf: 0.60 % (< =2); %LUC: 2.30 % (< =4); RBC: 4.59 10^6/µL (3.50-5.80); Hemogl: 14.30 g/dL (12.0-15.0); HTCO: 47.10 % (36.0-43.0)\\nVCM: 102.50 fL (78.0-100.0); HCM: 31.20 pg (27.0-32.0); CHCM: 30.50 g/dL (31.5-34.5); RDW-CV: 13.10 % (11.6-14.0); HDW: 2.78 g/dL (2.20-3.20); Plaquetas: 148.00 10^3/µL (130-450) Coagulación:  AP%: 107.40 % (80.0-120.0); INRa: 1.01 (< =1); APTT: 30.24 seg (25.0-35.0); Fi-der: 527.20 mg/dL (150.0-400.0); DDi: 1339.00 ng/ml (< =500) Bioquímica:  GLU: 82.00 mg/dl (70-110); URE: 76.00 mg/dl (10-50); CRE: 0.83 mg/dl (0.50-1.10); ALB: 3.20 g/dl (3.5-5.2); CA: 9.40 mg/dl (8.5-10.5); CAc: 10.3 mg/dl (8.6-10.2); NA: 139.00 mmol/L (135-147); K: 3.80 mmol/L (3.5-5.0); CL: 103.00 mmol/L (95-106); BT: 0.50 mg/dl (0.2-1.0); MIOG: 195.00 ng/ml (19.0-51.0); CK: 146.00 U/L (< =190); TNI: < 0.02 ng/ml (< =0); LDH: 288.00 U/L (80-235); GPT: 27.00 U/L (< =41); GOT: 28.00 U/L (< =31); FAL: 150.00 U/L (35-104) AMI: 29.00 U/L (< =100)\\n _FECHA_XXXX_  MN  Perfusión  pulmonar : Se  ha  realizado  gammagrafía  pulmonar,  despues  de  la  administración  por  vía  intravenosa  del  radiotrazador,  obteniendo  imágenes  planares,  en  proyecciones  anterior,  posterior,  laterales  y  oblicuas.  \\nPerfusión  pulmonar  homogénea  en  todos  los  campos,  sin  defectos  localizados  que  sugieran  TEP  en  la  actualidad. \\nResumen: Gammagrafía  pulmonar  sin  hallazgos  patológicos  significativos.  Se  descarta  razonablemente  la  existencia  de  TEP  en  la  actualidad. \\n11.09.2017Ecografía  Doppler  troncos  supraaórticos \\nTerritorio  carotídeo  DERECHO  de  características  morfológicas  normales.    No  hay  evidencia  de  ateromatosis  significativa.    Las  velocidades  y  curvas  espectrales  de  la  ACC,  ACI  y  ACE  están  dentro  de  límites  normales.    Se  detecta  flujo  en  la  arteria  vertebral  en  sentido  fisiológico.  Ratio  ACI/ACC:    0.64 \\nTerritorio  carotídeo  IZQUIERDO  de  características  morfológicas  normales.    No  hay  evidencia  de  ateromatosis  significativa.    Las  velocidades  y  curvas  espectrales  de  la  ACC,  ACI  y  ACE  están  dentro  de  límites  normales.    Se  detecta  flujo  en  la  arteria  vertebral  en  sentido  fisiológico.  Ratio  ACI/ACC:  1.32 \\nCONCLUSIÓN:  Sin  hallazgos  patológicos.\\n11/09/17-RX HOMBRO AP Y AXIAL: Sin alteraciones en marco óseo.\\n11/09/17Holter ECG: Ritmo sinusal con Fc media de 70, mínima de 48 y máxima de 98 lpm. 2 Extrasistoles ventriculares de 2 morfologías. 37 Extrasistoles supraventriculares. Asintomática durante el registro electrocardiográfico.\\n14-09-17-ESTUDIO EEG DE VIGILIA: Actividad de fondo simétrica, con ritmo alfa parieto occipital a 8 Hz. Ritmos rápidos de distribución fronto rolándica. \\nInterconsultas\\nAsuntos Sociales\\nMujer de 88 años de edad, viuda hace 20 años.Vive sola.\\nHijo único  _LOCALIDAD_XXXX_  móvil de contacto  _DATA_XXXX_  y nuera  _DATA_XXXX_ .\\nTramitada solicitud de reconocimiento de la ley de dependencia desestimada en  _FECHA_XXXX_  de 2014.Servicios sociales El  _NOMBRE_XXXX_ .\\nCoordinación para nueva cita e iniciar tramites de nuevo cita  _FECHA_XXXX_   a las  _HORA_XXXX_  horas.Entrego solicitud.\\nEntrego listado de empresas de asesoramiento de recursos y servicio de ayuda a domicilio.   \\nComentarios\\nMujer de 88 años que acude por episodio de pérdida de conciencia con pródromos previo, no observada por terceras personas, en paciente con betabloqueo como tratamiento antihipertensivo.  No datos indirectos de crisis tónico clónica (no pérdida control esfínteres o mordedura de lengua). No refiere disnea, palpitaciones o dolor torácico previo. Estos episodios han ocurrido en varias ocasiones previamente al episodio actual aunque no había consultado por ello. En urgencias no se objetivan taquicardias ni hipotensión, el EKG muestra un ritmo sinusal a 51 lpm, sin alteraciones del PR, QRS ni intervalo QT. Tampoco alteraciones destacables en placa de torax. No existen soplos sugerentes de estenosis aórtica severa y tiene un ecocardio previo sin valvulopatías. El TAC craneal no revela alteraciones significativas. Los marcadores enzimáticos cardíacos son normales. Gammagrafía de perfusión que descarta TEP. Se ingresa para estudio con retirada de betabloqueantes. En planta se completa el estudio con Doppler de TSA que no muestra alteraciones, un Holter de EKG sin alteraciones que justifiquen sincope (RS con frecuencia minima nocturna de 48 y máxima diurna de 98, con 2 extrasístoles ventriculares y 37 extrasístoles supraventriculares no asociados a evento clínico) y un electroencefalografía sin datos de actividad. Durante su ingreso no ha presentado clínica de mareo o síncopes y ha comenzado a caminar sin aparición de síntomas. Se concluye como hipótesis de los sincopes el tratamiento con betabloqueantes por lo que se suspenden. Las tensiones durante el ingreso han oscilado desde sistólicas de 125 o sistólicas de 155, por lo que se añade al tratamiento iecas a bajas dosis (enalapril 5 mg al día). Control clínico y de tensión arterial por su médico de primaria. Por otra parte, se evidenció en urgencias un sedimento de orina patológico con urocultivo postivo para E.Colli, tratándose la infección de orina con ciprofloxacino oral.\\nLa paciente tiene una Marcha de precaución por su edad que podría aumentar el riesgo de caídas por lo que se recomienda marcha con bastón, u andador, evitar barreras físicas en domicilio y supervivisión lo que se pone en conocimiento de la familia.\\nJuicio Clínico\\n-Síncope posiblemente hipotensivo y con bradicardia por toma de carvedilol. \\nDesacondicionamiento funcional progresivo.\\n-ITU por E. coli.\\nLos previos: HTA, Cardiopatía Hipertensiva, Deterioro cognitivo leve, Sd depresivo.\\nTratamiento\\nAÑADIRÁ A SU TRATAMIENTO PREVIO\\n· ENALAPRIL 5 mg: 1-0-0 (un comprimido en el desayuno).\\nSUSPENDERÁ DE SU TRATAMIENTO PREVIO\\n· COROPRES 25 1-0-0. \\nCONTINUARÁ CON EL RESTO DE SU TRATAMIENTO PREVIO\\n· DONEPEZILO 10. METAMIZOL (SI PRECISA). OMEPRAZOL 20 1-0-0. SERTRALINA 50mg: 1-0-0,5. TRAMADOL/PARACETAMOL (SI PRECISA)\\nRecomendaciones\\n\\tEn cumplimiento del artículo 5 de la LOPD  _FECHA_XXXX_ , se informa que sus datos identificativos y de salud serán objeto de tratamiento e incorporados a los ficheros de datos sanitarios, cuya titularidad corresponde al  _HOSPITAL_XXXX_ . Los datos únicamente serán utilizados con fines asociados a la atención y gestión sanitaria, docencia y seguimiento asistencial, estando prevista su comunicación a los organismos públicos con competencia en materia sanitaria. \\nEl órgano ante el que podrá ejercer los derechos de acceso, cancelación, rectificación y oposición de datos es el Servicio de Atención al Usuario del  _HOSPITAL_XXXX_ ,   _DIRECCION_XXXX_ \\n'}"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["dataset['train'].features"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w6G4aKeCCmWA","executionInfo":{"status":"ok","timestamp":1679507256221,"user_tz":0,"elapsed":11,"user":{"displayName":"Alexis Domínguez","userId":"12545493388305103028"}},"outputId":"2fa3026d-e2d5-4654-8c8d-ba0d21c5b0c5"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'acto': Value(dtype='int64', id=None),\n"," 'label': ClassLabel(names=['T38.0X5A', 'Y83.1', 'P02.69', 'T45.515A', 'T36.95XA', 'T50.2X5A', 'T39.315A', 'T39.95XA', 'O91.22', 'T86.10', 'Y95', 'T39.395A', 'T84.54XA', 'T84.226A', 'P01.1', 'K91.840', 'J95.811', 'T82.7XXA', 'T43.505A', 'T50.8X5A', 'E89.0', 'T45.1X5A', 'K94.29', 'T81.83XA', 'T45.615A', 'T86.5', 'K91.841', 'T88.8XXA', 'T80.1XXA', 'L76.22', 'O75.2', 'T46.0X5A', 'I97.190', 'O04.5', 'T81.4XXA', 'I95.2', 'T88.59XA', 'T36.1X5A', 'M96.1', 'T42.8X5A', 'O75.82', 'I97.121', 'T83.51XA', 'Y83.6', 'K66.0', 'T46.2X5A', 'Y84.2', 'L76.32', 'P03.89', 'K91.71', 'T84.428A', 'T85.49XA', 'T50.905A', 'G72.0', 'T83.89XA', 'T86.11', 'T42.4X5A', 'P03.4', 'K94.09', 'T80.212A', 'T39.2X5A', 'Y83.8', 'T39.4X5A', 'K12.31', 'P39.3', 'T82.09XD', 'K91.89', 'T82.9XXA', 'T82.868A', 'T42.75XA', 'G89.18', 'E89.89', 'R50.82', 'T50.3X5A', 'T40.2X5A', 'T82.524A', 'P13.4', 'T83.59XA', 'P02.7', 'T82.855D', 'T50.1X5A', 'P15.8', 'I97.51', 'T45.525A', 'N99.111', 'K91.61', 'T82.857A', 'D70.1', 'N99.512', 'P36.9', 'I97.790', 'T43.595A', 'T82.858A', 'T39.1X5A', 'T50.995A', 'P39.1', 'T84.84XA', 'T81.72XA', 'T84.89XA', 'O90.89', 'T80.219A', 'T82.838A', 'T36.8X5A', 'J95.830', 'K94.23', 'L27.1', 'T46.5X5A', 'G62.0', 'T83.098A', 'P39.9', 'E09.65', 'T47.1X5A', 'T80.211A', 'T49.0X5A', 'T81.83XD', 'O90.81', 'T40.4X5A', 'L27.0', 'T85.71XA', 'T38.0X5S', 'Y84.8', 'T36.5X5A', 'T38.0X5D', 'T82.120A', 'T38.3X5A', 'T85.590A', 'T79.7XXA', 'M96.830', 'T87.81', 'T44.995A', 'T81.31XA', 'T46.6X5A', 'T46.2X5S', 'T42.6X5A', 'P02.5', 'Y83.3', 'T82.310A', 'D61.1', 'T85.22XA', 'O89.4', 'P12.81', 'O86.12', 'Y84.0', 'P39.4', 'K91.2', 'N99.61', 'N99.71', 'T43.505S', 'Y83.5', 'G25.79', 'Y84.6', 'T85.79XA', 'O86.4', 'T84.021A', 'T45.4X5A', 'K94.19', 'I97.89', 'I97.52', 'T50.6X5D', 'G97.1', 'K12.32', 'T79.6XXA', 'T87.44', 'Y83.2', 'T81.19XA', 'H59.88', 'T86.831', 'P03.2', 'H59.022', 'T37.5X5A', 'P02.8', 'I97.2', 'T84.098A', 'N99.3', 'T84.296A', 'T81.12XA', 'T39.015A', 'T48.6X5A', 'K91.3', 'T84.033A', 'T85.89XA', 'T81.32XA', 'P13.2', 'K94.22', 'D70.2', 'T83.198A', 'G97.41', 'N99.89', 'T44.8X5A', 'H59.021', 'G97.49', 'T82.09XA', 'T50.Z15A', 'T85.44XA', 'O90.0', 'T83.021A', 'T50.0X5A', 'T85.398A', 'T84.498A', 'T84.82XA', 'I97.618', 'T40.605A', 'N99.820', 'L76.02', 'G25.1', 'L76.31', 'T47.4X5A', 'N99.821', 'T36.0X5A', 'T41.5X5A', 'T84.223A', 'T45.1X5D', 'T83.498A', 'T43.295A', 'K94.12', 'P39.8', 'O86.29', 'M96.840', 'I97.130', 'T84.51XA', 'Y64.0', 'G97.51', 'T82.330A', 'T83.29XA', 'E89.2', 'E36.01', 'T84.060A', 'G97.82', 'K95.81', 'T85.29XA', 'T82.190A', 'T86.19', 'T81.89XA', 'P01.5', 'G89.28', 'M1A.20X0', 'K91.72', 'K94.21', 'T37.95XA', 'T83.83XA', 'T85.611A', 'T80.92XA', 'K85.30', 'T38.895A', 'I97.710', 'T79.A3XA', 'T37.1X5S', 'P01.2', 'T80.89XA', 'T50.2X5D', 'N99.528', 'T84.59XA', 'T45.1X5S', 'T84.011A', 'T37.1X5A', 'N98.1', 'T44.7X5A', 'T88.7XXA', 'T85.598A', 'Y65.8', 'T85.85XA', 'T82.855A', 'K59.03', 'T43.595S', 'T45.8X5A', 'M96.89', 'T86.01', 'D64.81', 'T83.511A', 'T82.898A', 'T84.63XA', 'T37.2X5A', 'N99.532', 'J95.3', 'T84.030A', 'T82.538A', 'M96.0', 'K91.1', 'T39.395S', 'O85', 'K94.13', 'T82.594A', 'T84.490A', 'N99.522', 'G97.52', 'T46.4X5A', 'Y83.0', 'J95.61', 'T48.4X5A', 'T86.12', 'L76.21', 'T38.4X5A', 'T44.995D', 'H95.01', 'T85.828A', 'T85.698A', 'M96.810', 'T50.4X5A', 'G21.11', 'Y84.1', 'D61.810', 'T82.858S', 'T84.194A', 'N14.2', 'T87.40', 'T84.9XXA', 'T43.205A', 'G21.19', 'N99.72', 'P03.3', 'T85.86XA', 'D61.811', 'T87.89', 'T82.818A', 'T88.0XXA', 'G24.01', 'T84.020A', 'T80.218A', 'J95.851', 'T43.95XA', 'T50.8X5S', 'T84.091A', 'T36.3X5A', 'N52.34', 'N99.521', 'T42.1X5A', 'I97.610', 'P04.1', 'G25.71', 'T38.2X5S', 'N99.511', 'T43.225A', 'T84.032A', 'T41.45XA', 'T84.038A', 'T88.6XXA', 'N99.530', 'T84.216A', 'I95.81', 'T45.625A', 'T84.029A', 'T83.091A', 'M96.842', 'T84.69XA', 'T84.50XA', 'T82.119A', 'T82.6XXA', 'T82.856A', 'T83.9XXA', 'T82.897A', 'I97.191', 'T84.620A', 'T38.1X5A', 'I97.418', 'N99.114', 'T82.848A', 'T83.191A', 'T84.090A', 'T82.199A', 'K91.850', 'O08.1', 'T82.110A', 'T82.223D', 'J95.03', 'T85.122A', 'T85.898A', 'N99.113', 'T84.049A', 'I97.638', 'T81.4XXS', 'T46.2X5D', 'L76.82', 'T84.124A', 'P38.9', 'T84.021D', 'T82.320A', 'T88.8XXS', 'T84.7XXA', 'J95.89', 'T82.120D', 'N99.112', 'T86.09', 'T37.8X5A', 'T81.718A', 'P36.8', 'N99.110', 'T84.022A', 'P02.1', 'O75.1', 'T85.520A', 'T43.225D', 'E09.9', 'T84.52XA', 'T43.595D', 'T81.599A', 'Y84.9', 'T82.867A', 'T85.09XA', 'T84.53XA', 'T84.013A', 'T84.048A', 'T83.028A', 'P01.8', 'T43.505D', 'N52.37', 'N99.841', 'T80.29XA', 'T50.8X5D', 'T43.025A', 'T84.114A', 'T80.818A', 'N99.0', 'O90.2', 'P14.1', 'T83.420A', 'O74.8', 'T84.040A', 'N99.83', 'I97.611', 'T83.728S', 'H21.81', 'P12.89', 'T83.6XXA', 'P36.2', 'Y84.5', 'T45.7X5A', 'M96.69', 'P03.810', 'T41.3X5A', 'E89.3', 'T84.031A', 'O91.12', 'E89.6', 'T82.03XD'], id=None),\n"," 'label_str': Value(dtype='string', id=None),\n"," 'labels_str': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n"," 'informes': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n"," 'text': Value(dtype='string', id=None)}"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["The dataset consists of tweets, labeled with one or more emotions. \n","\n","Let's create a list that contains the labels, as well as 2 dictionaries that map labels to integers and back."],"metadata":{"id":"P-PabaGoC4Oq"}},{"cell_type":"code","source":["from datasets import ClassLabel\n","\n","class2label = dataset['train'].features[\"label\"]\n","id2label = {idx:label for idx, label in enumerate(class2label._int2str)}\n","label2id = class2label._str2int\n","\n","print(class2label)\n","print(id2label)\n","print(label2id)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PLpNdkaYC5ID","executionInfo":{"status":"ok","timestamp":1679507256222,"user_tz":0,"elapsed":8,"user":{"displayName":"Alexis Domínguez","userId":"12545493388305103028"}},"outputId":"a646613b-4eaa-423b-bcc3-62915a1c7506"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["ClassLabel(names=['T38.0X5A', 'Y83.1', 'P02.69', 'T45.515A', 'T36.95XA', 'T50.2X5A', 'T39.315A', 'T39.95XA', 'O91.22', 'T86.10', 'Y95', 'T39.395A', 'T84.54XA', 'T84.226A', 'P01.1', 'K91.840', 'J95.811', 'T82.7XXA', 'T43.505A', 'T50.8X5A', 'E89.0', 'T45.1X5A', 'K94.29', 'T81.83XA', 'T45.615A', 'T86.5', 'K91.841', 'T88.8XXA', 'T80.1XXA', 'L76.22', 'O75.2', 'T46.0X5A', 'I97.190', 'O04.5', 'T81.4XXA', 'I95.2', 'T88.59XA', 'T36.1X5A', 'M96.1', 'T42.8X5A', 'O75.82', 'I97.121', 'T83.51XA', 'Y83.6', 'K66.0', 'T46.2X5A', 'Y84.2', 'L76.32', 'P03.89', 'K91.71', 'T84.428A', 'T85.49XA', 'T50.905A', 'G72.0', 'T83.89XA', 'T86.11', 'T42.4X5A', 'P03.4', 'K94.09', 'T80.212A', 'T39.2X5A', 'Y83.8', 'T39.4X5A', 'K12.31', 'P39.3', 'T82.09XD', 'K91.89', 'T82.9XXA', 'T82.868A', 'T42.75XA', 'G89.18', 'E89.89', 'R50.82', 'T50.3X5A', 'T40.2X5A', 'T82.524A', 'P13.4', 'T83.59XA', 'P02.7', 'T82.855D', 'T50.1X5A', 'P15.8', 'I97.51', 'T45.525A', 'N99.111', 'K91.61', 'T82.857A', 'D70.1', 'N99.512', 'P36.9', 'I97.790', 'T43.595A', 'T82.858A', 'T39.1X5A', 'T50.995A', 'P39.1', 'T84.84XA', 'T81.72XA', 'T84.89XA', 'O90.89', 'T80.219A', 'T82.838A', 'T36.8X5A', 'J95.830', 'K94.23', 'L27.1', 'T46.5X5A', 'G62.0', 'T83.098A', 'P39.9', 'E09.65', 'T47.1X5A', 'T80.211A', 'T49.0X5A', 'T81.83XD', 'O90.81', 'T40.4X5A', 'L27.0', 'T85.71XA', 'T38.0X5S', 'Y84.8', 'T36.5X5A', 'T38.0X5D', 'T82.120A', 'T38.3X5A', 'T85.590A', 'T79.7XXA', 'M96.830', 'T87.81', 'T44.995A', 'T81.31XA', 'T46.6X5A', 'T46.2X5S', 'T42.6X5A', 'P02.5', 'Y83.3', 'T82.310A', 'D61.1', 'T85.22XA', 'O89.4', 'P12.81', 'O86.12', 'Y84.0', 'P39.4', 'K91.2', 'N99.61', 'N99.71', 'T43.505S', 'Y83.5', 'G25.79', 'Y84.6', 'T85.79XA', 'O86.4', 'T84.021A', 'T45.4X5A', 'K94.19', 'I97.89', 'I97.52', 'T50.6X5D', 'G97.1', 'K12.32', 'T79.6XXA', 'T87.44', 'Y83.2', 'T81.19XA', 'H59.88', 'T86.831', 'P03.2', 'H59.022', 'T37.5X5A', 'P02.8', 'I97.2', 'T84.098A', 'N99.3', 'T84.296A', 'T81.12XA', 'T39.015A', 'T48.6X5A', 'K91.3', 'T84.033A', 'T85.89XA', 'T81.32XA', 'P13.2', 'K94.22', 'D70.2', 'T83.198A', 'G97.41', 'N99.89', 'T44.8X5A', 'H59.021', 'G97.49', 'T82.09XA', 'T50.Z15A', 'T85.44XA', 'O90.0', 'T83.021A', 'T50.0X5A', 'T85.398A', 'T84.498A', 'T84.82XA', 'I97.618', 'T40.605A', 'N99.820', 'L76.02', 'G25.1', 'L76.31', 'T47.4X5A', 'N99.821', 'T36.0X5A', 'T41.5X5A', 'T84.223A', 'T45.1X5D', 'T83.498A', 'T43.295A', 'K94.12', 'P39.8', 'O86.29', 'M96.840', 'I97.130', 'T84.51XA', 'Y64.0', 'G97.51', 'T82.330A', 'T83.29XA', 'E89.2', 'E36.01', 'T84.060A', 'G97.82', 'K95.81', 'T85.29XA', 'T82.190A', 'T86.19', 'T81.89XA', 'P01.5', 'G89.28', 'M1A.20X0', 'K91.72', 'K94.21', 'T37.95XA', 'T83.83XA', 'T85.611A', 'T80.92XA', 'K85.30', 'T38.895A', 'I97.710', 'T79.A3XA', 'T37.1X5S', 'P01.2', 'T80.89XA', 'T50.2X5D', 'N99.528', 'T84.59XA', 'T45.1X5S', 'T84.011A', 'T37.1X5A', 'N98.1', 'T44.7X5A', 'T88.7XXA', 'T85.598A', 'Y65.8', 'T85.85XA', 'T82.855A', 'K59.03', 'T43.595S', 'T45.8X5A', 'M96.89', 'T86.01', 'D64.81', 'T83.511A', 'T82.898A', 'T84.63XA', 'T37.2X5A', 'N99.532', 'J95.3', 'T84.030A', 'T82.538A', 'M96.0', 'K91.1', 'T39.395S', 'O85', 'K94.13', 'T82.594A', 'T84.490A', 'N99.522', 'G97.52', 'T46.4X5A', 'Y83.0', 'J95.61', 'T48.4X5A', 'T86.12', 'L76.21', 'T38.4X5A', 'T44.995D', 'H95.01', 'T85.828A', 'T85.698A', 'M96.810', 'T50.4X5A', 'G21.11', 'Y84.1', 'D61.810', 'T82.858S', 'T84.194A', 'N14.2', 'T87.40', 'T84.9XXA', 'T43.205A', 'G21.19', 'N99.72', 'P03.3', 'T85.86XA', 'D61.811', 'T87.89', 'T82.818A', 'T88.0XXA', 'G24.01', 'T84.020A', 'T80.218A', 'J95.851', 'T43.95XA', 'T50.8X5S', 'T84.091A', 'T36.3X5A', 'N52.34', 'N99.521', 'T42.1X5A', 'I97.610', 'P04.1', 'G25.71', 'T38.2X5S', 'N99.511', 'T43.225A', 'T84.032A', 'T41.45XA', 'T84.038A', 'T88.6XXA', 'N99.530', 'T84.216A', 'I95.81', 'T45.625A', 'T84.029A', 'T83.091A', 'M96.842', 'T84.69XA', 'T84.50XA', 'T82.119A', 'T82.6XXA', 'T82.856A', 'T83.9XXA', 'T82.897A', 'I97.191', 'T84.620A', 'T38.1X5A', 'I97.418', 'N99.114', 'T82.848A', 'T83.191A', 'T84.090A', 'T82.199A', 'K91.850', 'O08.1', 'T82.110A', 'T82.223D', 'J95.03', 'T85.122A', 'T85.898A', 'N99.113', 'T84.049A', 'I97.638', 'T81.4XXS', 'T46.2X5D', 'L76.82', 'T84.124A', 'P38.9', 'T84.021D', 'T82.320A', 'T88.8XXS', 'T84.7XXA', 'J95.89', 'T82.120D', 'N99.112', 'T86.09', 'T37.8X5A', 'T81.718A', 'P36.8', 'N99.110', 'T84.022A', 'P02.1', 'O75.1', 'T85.520A', 'T43.225D', 'E09.9', 'T84.52XA', 'T43.595D', 'T81.599A', 'Y84.9', 'T82.867A', 'T85.09XA', 'T84.53XA', 'T84.013A', 'T84.048A', 'T83.028A', 'P01.8', 'T43.505D', 'N52.37', 'N99.841', 'T80.29XA', 'T50.8X5D', 'T43.025A', 'T84.114A', 'T80.818A', 'N99.0', 'O90.2', 'P14.1', 'T83.420A', 'O74.8', 'T84.040A', 'N99.83', 'I97.611', 'T83.728S', 'H21.81', 'P12.89', 'T83.6XXA', 'P36.2', 'Y84.5', 'T45.7X5A', 'M96.69', 'P03.810', 'T41.3X5A', 'E89.3', 'T84.031A', 'O91.12', 'E89.6', 'T82.03XD'], id=None)\n","{0: 'T38.0X5A', 1: 'Y83.1', 2: 'P02.69', 3: 'T45.515A', 4: 'T36.95XA', 5: 'T50.2X5A', 6: 'T39.315A', 7: 'T39.95XA', 8: 'O91.22', 9: 'T86.10', 10: 'Y95', 11: 'T39.395A', 12: 'T84.54XA', 13: 'T84.226A', 14: 'P01.1', 15: 'K91.840', 16: 'J95.811', 17: 'T82.7XXA', 18: 'T43.505A', 19: 'T50.8X5A', 20: 'E89.0', 21: 'T45.1X5A', 22: 'K94.29', 23: 'T81.83XA', 24: 'T45.615A', 25: 'T86.5', 26: 'K91.841', 27: 'T88.8XXA', 28: 'T80.1XXA', 29: 'L76.22', 30: 'O75.2', 31: 'T46.0X5A', 32: 'I97.190', 33: 'O04.5', 34: 'T81.4XXA', 35: 'I95.2', 36: 'T88.59XA', 37: 'T36.1X5A', 38: 'M96.1', 39: 'T42.8X5A', 40: 'O75.82', 41: 'I97.121', 42: 'T83.51XA', 43: 'Y83.6', 44: 'K66.0', 45: 'T46.2X5A', 46: 'Y84.2', 47: 'L76.32', 48: 'P03.89', 49: 'K91.71', 50: 'T84.428A', 51: 'T85.49XA', 52: 'T50.905A', 53: 'G72.0', 54: 'T83.89XA', 55: 'T86.11', 56: 'T42.4X5A', 57: 'P03.4', 58: 'K94.09', 59: 'T80.212A', 60: 'T39.2X5A', 61: 'Y83.8', 62: 'T39.4X5A', 63: 'K12.31', 64: 'P39.3', 65: 'T82.09XD', 66: 'K91.89', 67: 'T82.9XXA', 68: 'T82.868A', 69: 'T42.75XA', 70: 'G89.18', 71: 'E89.89', 72: 'R50.82', 73: 'T50.3X5A', 74: 'T40.2X5A', 75: 'T82.524A', 76: 'P13.4', 77: 'T83.59XA', 78: 'P02.7', 79: 'T82.855D', 80: 'T50.1X5A', 81: 'P15.8', 82: 'I97.51', 83: 'T45.525A', 84: 'N99.111', 85: 'K91.61', 86: 'T82.857A', 87: 'D70.1', 88: 'N99.512', 89: 'P36.9', 90: 'I97.790', 91: 'T43.595A', 92: 'T82.858A', 93: 'T39.1X5A', 94: 'T50.995A', 95: 'P39.1', 96: 'T84.84XA', 97: 'T81.72XA', 98: 'T84.89XA', 99: 'O90.89', 100: 'T80.219A', 101: 'T82.838A', 102: 'T36.8X5A', 103: 'J95.830', 104: 'K94.23', 105: 'L27.1', 106: 'T46.5X5A', 107: 'G62.0', 108: 'T83.098A', 109: 'P39.9', 110: 'E09.65', 111: 'T47.1X5A', 112: 'T80.211A', 113: 'T49.0X5A', 114: 'T81.83XD', 115: 'O90.81', 116: 'T40.4X5A', 117: 'L27.0', 118: 'T85.71XA', 119: 'T38.0X5S', 120: 'Y84.8', 121: 'T36.5X5A', 122: 'T38.0X5D', 123: 'T82.120A', 124: 'T38.3X5A', 125: 'T85.590A', 126: 'T79.7XXA', 127: 'M96.830', 128: 'T87.81', 129: 'T44.995A', 130: 'T81.31XA', 131: 'T46.6X5A', 132: 'T46.2X5S', 133: 'T42.6X5A', 134: 'P02.5', 135: 'Y83.3', 136: 'T82.310A', 137: 'D61.1', 138: 'T85.22XA', 139: 'O89.4', 140: 'P12.81', 141: 'O86.12', 142: 'Y84.0', 143: 'P39.4', 144: 'K91.2', 145: 'N99.61', 146: 'N99.71', 147: 'T43.505S', 148: 'Y83.5', 149: 'G25.79', 150: 'Y84.6', 151: 'T85.79XA', 152: 'O86.4', 153: 'T84.021A', 154: 'T45.4X5A', 155: 'K94.19', 156: 'I97.89', 157: 'I97.52', 158: 'T50.6X5D', 159: 'G97.1', 160: 'K12.32', 161: 'T79.6XXA', 162: 'T87.44', 163: 'Y83.2', 164: 'T81.19XA', 165: 'H59.88', 166: 'T86.831', 167: 'P03.2', 168: 'H59.022', 169: 'T37.5X5A', 170: 'P02.8', 171: 'I97.2', 172: 'T84.098A', 173: 'N99.3', 174: 'T84.296A', 175: 'T81.12XA', 176: 'T39.015A', 177: 'T48.6X5A', 178: 'K91.3', 179: 'T84.033A', 180: 'T85.89XA', 181: 'T81.32XA', 182: 'P13.2', 183: 'K94.22', 184: 'D70.2', 185: 'T83.198A', 186: 'G97.41', 187: 'N99.89', 188: 'T44.8X5A', 189: 'H59.021', 190: 'G97.49', 191: 'T82.09XA', 192: 'T50.Z15A', 193: 'T85.44XA', 194: 'O90.0', 195: 'T83.021A', 196: 'T50.0X5A', 197: 'T85.398A', 198: 'T84.498A', 199: 'T84.82XA', 200: 'I97.618', 201: 'T40.605A', 202: 'N99.820', 203: 'L76.02', 204: 'G25.1', 205: 'L76.31', 206: 'T47.4X5A', 207: 'N99.821', 208: 'T36.0X5A', 209: 'T41.5X5A', 210: 'T84.223A', 211: 'T45.1X5D', 212: 'T83.498A', 213: 'T43.295A', 214: 'K94.12', 215: 'P39.8', 216: 'O86.29', 217: 'M96.840', 218: 'I97.130', 219: 'T84.51XA', 220: 'Y64.0', 221: 'G97.51', 222: 'T82.330A', 223: 'T83.29XA', 224: 'E89.2', 225: 'E36.01', 226: 'T84.060A', 227: 'G97.82', 228: 'K95.81', 229: 'T85.29XA', 230: 'T82.190A', 231: 'T86.19', 232: 'T81.89XA', 233: 'P01.5', 234: 'G89.28', 235: 'M1A.20X0', 236: 'K91.72', 237: 'K94.21', 238: 'T37.95XA', 239: 'T83.83XA', 240: 'T85.611A', 241: 'T80.92XA', 242: 'K85.30', 243: 'T38.895A', 244: 'I97.710', 245: 'T79.A3XA', 246: 'T37.1X5S', 247: 'P01.2', 248: 'T80.89XA', 249: 'T50.2X5D', 250: 'N99.528', 251: 'T84.59XA', 252: 'T45.1X5S', 253: 'T84.011A', 254: 'T37.1X5A', 255: 'N98.1', 256: 'T44.7X5A', 257: 'T88.7XXA', 258: 'T85.598A', 259: 'Y65.8', 260: 'T85.85XA', 261: 'T82.855A', 262: 'K59.03', 263: 'T43.595S', 264: 'T45.8X5A', 265: 'M96.89', 266: 'T86.01', 267: 'D64.81', 268: 'T83.511A', 269: 'T82.898A', 270: 'T84.63XA', 271: 'T37.2X5A', 272: 'N99.532', 273: 'J95.3', 274: 'T84.030A', 275: 'T82.538A', 276: 'M96.0', 277: 'K91.1', 278: 'T39.395S', 279: 'O85', 280: 'K94.13', 281: 'T82.594A', 282: 'T84.490A', 283: 'N99.522', 284: 'G97.52', 285: 'T46.4X5A', 286: 'Y83.0', 287: 'J95.61', 288: 'T48.4X5A', 289: 'T86.12', 290: 'L76.21', 291: 'T38.4X5A', 292: 'T44.995D', 293: 'H95.01', 294: 'T85.828A', 295: 'T85.698A', 296: 'M96.810', 297: 'T50.4X5A', 298: 'G21.11', 299: 'Y84.1', 300: 'D61.810', 301: 'T82.858S', 302: 'T84.194A', 303: 'N14.2', 304: 'T87.40', 305: 'T84.9XXA', 306: 'T43.205A', 307: 'G21.19', 308: 'N99.72', 309: 'P03.3', 310: 'T85.86XA', 311: 'D61.811', 312: 'T87.89', 313: 'T82.818A', 314: 'T88.0XXA', 315: 'G24.01', 316: 'T84.020A', 317: 'T80.218A', 318: 'J95.851', 319: 'T43.95XA', 320: 'T50.8X5S', 321: 'T84.091A', 322: 'T36.3X5A', 323: 'N52.34', 324: 'N99.521', 325: 'T42.1X5A', 326: 'I97.610', 327: 'P04.1', 328: 'G25.71', 329: 'T38.2X5S', 330: 'N99.511', 331: 'T43.225A', 332: 'T84.032A', 333: 'T41.45XA', 334: 'T84.038A', 335: 'T88.6XXA', 336: 'N99.530', 337: 'T84.216A', 338: 'I95.81', 339: 'T45.625A', 340: 'T84.029A', 341: 'T83.091A', 342: 'M96.842', 343: 'T84.69XA', 344: 'T84.50XA', 345: 'T82.119A', 346: 'T82.6XXA', 347: 'T82.856A', 348: 'T83.9XXA', 349: 'T82.897A', 350: 'I97.191', 351: 'T84.620A', 352: 'T38.1X5A', 353: 'I97.418', 354: 'N99.114', 355: 'T82.848A', 356: 'T83.191A', 357: 'T84.090A', 358: 'T82.199A', 359: 'K91.850', 360: 'O08.1', 361: 'T82.110A', 362: 'T82.223D', 363: 'J95.03', 364: 'T85.122A', 365: 'T85.898A', 366: 'N99.113', 367: 'T84.049A', 368: 'I97.638', 369: 'T81.4XXS', 370: 'T46.2X5D', 371: 'L76.82', 372: 'T84.124A', 373: 'P38.9', 374: 'T84.021D', 375: 'T82.320A', 376: 'T88.8XXS', 377: 'T84.7XXA', 378: 'J95.89', 379: 'T82.120D', 380: 'N99.112', 381: 'T86.09', 382: 'T37.8X5A', 383: 'T81.718A', 384: 'P36.8', 385: 'N99.110', 386: 'T84.022A', 387: 'P02.1', 388: 'O75.1', 389: 'T85.520A', 390: 'T43.225D', 391: 'E09.9', 392: 'T84.52XA', 393: 'T43.595D', 394: 'T81.599A', 395: 'Y84.9', 396: 'T82.867A', 397: 'T85.09XA', 398: 'T84.53XA', 399: 'T84.013A', 400: 'T84.048A', 401: 'T83.028A', 402: 'P01.8', 403: 'T43.505D', 404: 'N52.37', 405: 'N99.841', 406: 'T80.29XA', 407: 'T50.8X5D', 408: 'T43.025A', 409: 'T84.114A', 410: 'T80.818A', 411: 'N99.0', 412: 'O90.2', 413: 'P14.1', 414: 'T83.420A', 415: 'O74.8', 416: 'T84.040A', 417: 'N99.83', 418: 'I97.611', 419: 'T83.728S', 420: 'H21.81', 421: 'P12.89', 422: 'T83.6XXA', 423: 'P36.2', 424: 'Y84.5', 425: 'T45.7X5A', 426: 'M96.69', 427: 'P03.810', 428: 'T41.3X5A', 429: 'E89.3', 430: 'T84.031A', 431: 'O91.12', 432: 'E89.6', 433: 'T82.03XD'}\n","{'T38.0X5A': 0, 'Y83.1': 1, 'P02.69': 2, 'T45.515A': 3, 'T36.95XA': 4, 'T50.2X5A': 5, 'T39.315A': 6, 'T39.95XA': 7, 'O91.22': 8, 'T86.10': 9, 'Y95': 10, 'T39.395A': 11, 'T84.54XA': 12, 'T84.226A': 13, 'P01.1': 14, 'K91.840': 15, 'J95.811': 16, 'T82.7XXA': 17, 'T43.505A': 18, 'T50.8X5A': 19, 'E89.0': 20, 'T45.1X5A': 21, 'K94.29': 22, 'T81.83XA': 23, 'T45.615A': 24, 'T86.5': 25, 'K91.841': 26, 'T88.8XXA': 27, 'T80.1XXA': 28, 'L76.22': 29, 'O75.2': 30, 'T46.0X5A': 31, 'I97.190': 32, 'O04.5': 33, 'T81.4XXA': 34, 'I95.2': 35, 'T88.59XA': 36, 'T36.1X5A': 37, 'M96.1': 38, 'T42.8X5A': 39, 'O75.82': 40, 'I97.121': 41, 'T83.51XA': 42, 'Y83.6': 43, 'K66.0': 44, 'T46.2X5A': 45, 'Y84.2': 46, 'L76.32': 47, 'P03.89': 48, 'K91.71': 49, 'T84.428A': 50, 'T85.49XA': 51, 'T50.905A': 52, 'G72.0': 53, 'T83.89XA': 54, 'T86.11': 55, 'T42.4X5A': 56, 'P03.4': 57, 'K94.09': 58, 'T80.212A': 59, 'T39.2X5A': 60, 'Y83.8': 61, 'T39.4X5A': 62, 'K12.31': 63, 'P39.3': 64, 'T82.09XD': 65, 'K91.89': 66, 'T82.9XXA': 67, 'T82.868A': 68, 'T42.75XA': 69, 'G89.18': 70, 'E89.89': 71, 'R50.82': 72, 'T50.3X5A': 73, 'T40.2X5A': 74, 'T82.524A': 75, 'P13.4': 76, 'T83.59XA': 77, 'P02.7': 78, 'T82.855D': 79, 'T50.1X5A': 80, 'P15.8': 81, 'I97.51': 82, 'T45.525A': 83, 'N99.111': 84, 'K91.61': 85, 'T82.857A': 86, 'D70.1': 87, 'N99.512': 88, 'P36.9': 89, 'I97.790': 90, 'T43.595A': 91, 'T82.858A': 92, 'T39.1X5A': 93, 'T50.995A': 94, 'P39.1': 95, 'T84.84XA': 96, 'T81.72XA': 97, 'T84.89XA': 98, 'O90.89': 99, 'T80.219A': 100, 'T82.838A': 101, 'T36.8X5A': 102, 'J95.830': 103, 'K94.23': 104, 'L27.1': 105, 'T46.5X5A': 106, 'G62.0': 107, 'T83.098A': 108, 'P39.9': 109, 'E09.65': 110, 'T47.1X5A': 111, 'T80.211A': 112, 'T49.0X5A': 113, 'T81.83XD': 114, 'O90.81': 115, 'T40.4X5A': 116, 'L27.0': 117, 'T85.71XA': 118, 'T38.0X5S': 119, 'Y84.8': 120, 'T36.5X5A': 121, 'T38.0X5D': 122, 'T82.120A': 123, 'T38.3X5A': 124, 'T85.590A': 125, 'T79.7XXA': 126, 'M96.830': 127, 'T87.81': 128, 'T44.995A': 129, 'T81.31XA': 130, 'T46.6X5A': 131, 'T46.2X5S': 132, 'T42.6X5A': 133, 'P02.5': 134, 'Y83.3': 135, 'T82.310A': 136, 'D61.1': 137, 'T85.22XA': 138, 'O89.4': 139, 'P12.81': 140, 'O86.12': 141, 'Y84.0': 142, 'P39.4': 143, 'K91.2': 144, 'N99.61': 145, 'N99.71': 146, 'T43.505S': 147, 'Y83.5': 148, 'G25.79': 149, 'Y84.6': 150, 'T85.79XA': 151, 'O86.4': 152, 'T84.021A': 153, 'T45.4X5A': 154, 'K94.19': 155, 'I97.89': 156, 'I97.52': 157, 'T50.6X5D': 158, 'G97.1': 159, 'K12.32': 160, 'T79.6XXA': 161, 'T87.44': 162, 'Y83.2': 163, 'T81.19XA': 164, 'H59.88': 165, 'T86.831': 166, 'P03.2': 167, 'H59.022': 168, 'T37.5X5A': 169, 'P02.8': 170, 'I97.2': 171, 'T84.098A': 172, 'N99.3': 173, 'T84.296A': 174, 'T81.12XA': 175, 'T39.015A': 176, 'T48.6X5A': 177, 'K91.3': 178, 'T84.033A': 179, 'T85.89XA': 180, 'T81.32XA': 181, 'P13.2': 182, 'K94.22': 183, 'D70.2': 184, 'T83.198A': 185, 'G97.41': 186, 'N99.89': 187, 'T44.8X5A': 188, 'H59.021': 189, 'G97.49': 190, 'T82.09XA': 191, 'T50.Z15A': 192, 'T85.44XA': 193, 'O90.0': 194, 'T83.021A': 195, 'T50.0X5A': 196, 'T85.398A': 197, 'T84.498A': 198, 'T84.82XA': 199, 'I97.618': 200, 'T40.605A': 201, 'N99.820': 202, 'L76.02': 203, 'G25.1': 204, 'L76.31': 205, 'T47.4X5A': 206, 'N99.821': 207, 'T36.0X5A': 208, 'T41.5X5A': 209, 'T84.223A': 210, 'T45.1X5D': 211, 'T83.498A': 212, 'T43.295A': 213, 'K94.12': 214, 'P39.8': 215, 'O86.29': 216, 'M96.840': 217, 'I97.130': 218, 'T84.51XA': 219, 'Y64.0': 220, 'G97.51': 221, 'T82.330A': 222, 'T83.29XA': 223, 'E89.2': 224, 'E36.01': 225, 'T84.060A': 226, 'G97.82': 227, 'K95.81': 228, 'T85.29XA': 229, 'T82.190A': 230, 'T86.19': 231, 'T81.89XA': 232, 'P01.5': 233, 'G89.28': 234, 'M1A.20X0': 235, 'K91.72': 236, 'K94.21': 237, 'T37.95XA': 238, 'T83.83XA': 239, 'T85.611A': 240, 'T80.92XA': 241, 'K85.30': 242, 'T38.895A': 243, 'I97.710': 244, 'T79.A3XA': 245, 'T37.1X5S': 246, 'P01.2': 247, 'T80.89XA': 248, 'T50.2X5D': 249, 'N99.528': 250, 'T84.59XA': 251, 'T45.1X5S': 252, 'T84.011A': 253, 'T37.1X5A': 254, 'N98.1': 255, 'T44.7X5A': 256, 'T88.7XXA': 257, 'T85.598A': 258, 'Y65.8': 259, 'T85.85XA': 260, 'T82.855A': 261, 'K59.03': 262, 'T43.595S': 263, 'T45.8X5A': 264, 'M96.89': 265, 'T86.01': 266, 'D64.81': 267, 'T83.511A': 268, 'T82.898A': 269, 'T84.63XA': 270, 'T37.2X5A': 271, 'N99.532': 272, 'J95.3': 273, 'T84.030A': 274, 'T82.538A': 275, 'M96.0': 276, 'K91.1': 277, 'T39.395S': 278, 'O85': 279, 'K94.13': 280, 'T82.594A': 281, 'T84.490A': 282, 'N99.522': 283, 'G97.52': 284, 'T46.4X5A': 285, 'Y83.0': 286, 'J95.61': 287, 'T48.4X5A': 288, 'T86.12': 289, 'L76.21': 290, 'T38.4X5A': 291, 'T44.995D': 292, 'H95.01': 293, 'T85.828A': 294, 'T85.698A': 295, 'M96.810': 296, 'T50.4X5A': 297, 'G21.11': 298, 'Y84.1': 299, 'D61.810': 300, 'T82.858S': 301, 'T84.194A': 302, 'N14.2': 303, 'T87.40': 304, 'T84.9XXA': 305, 'T43.205A': 306, 'G21.19': 307, 'N99.72': 308, 'P03.3': 309, 'T85.86XA': 310, 'D61.811': 311, 'T87.89': 312, 'T82.818A': 313, 'T88.0XXA': 314, 'G24.01': 315, 'T84.020A': 316, 'T80.218A': 317, 'J95.851': 318, 'T43.95XA': 319, 'T50.8X5S': 320, 'T84.091A': 321, 'T36.3X5A': 322, 'N52.34': 323, 'N99.521': 324, 'T42.1X5A': 325, 'I97.610': 326, 'P04.1': 327, 'G25.71': 328, 'T38.2X5S': 329, 'N99.511': 330, 'T43.225A': 331, 'T84.032A': 332, 'T41.45XA': 333, 'T84.038A': 334, 'T88.6XXA': 335, 'N99.530': 336, 'T84.216A': 337, 'I95.81': 338, 'T45.625A': 339, 'T84.029A': 340, 'T83.091A': 341, 'M96.842': 342, 'T84.69XA': 343, 'T84.50XA': 344, 'T82.119A': 345, 'T82.6XXA': 346, 'T82.856A': 347, 'T83.9XXA': 348, 'T82.897A': 349, 'I97.191': 350, 'T84.620A': 351, 'T38.1X5A': 352, 'I97.418': 353, 'N99.114': 354, 'T82.848A': 355, 'T83.191A': 356, 'T84.090A': 357, 'T82.199A': 358, 'K91.850': 359, 'O08.1': 360, 'T82.110A': 361, 'T82.223D': 362, 'J95.03': 363, 'T85.122A': 364, 'T85.898A': 365, 'N99.113': 366, 'T84.049A': 367, 'I97.638': 368, 'T81.4XXS': 369, 'T46.2X5D': 370, 'L76.82': 371, 'T84.124A': 372, 'P38.9': 373, 'T84.021D': 374, 'T82.320A': 375, 'T88.8XXS': 376, 'T84.7XXA': 377, 'J95.89': 378, 'T82.120D': 379, 'N99.112': 380, 'T86.09': 381, 'T37.8X5A': 382, 'T81.718A': 383, 'P36.8': 384, 'N99.110': 385, 'T84.022A': 386, 'P02.1': 387, 'O75.1': 388, 'T85.520A': 389, 'T43.225D': 390, 'E09.9': 391, 'T84.52XA': 392, 'T43.595D': 393, 'T81.599A': 394, 'Y84.9': 395, 'T82.867A': 396, 'T85.09XA': 397, 'T84.53XA': 398, 'T84.013A': 399, 'T84.048A': 400, 'T83.028A': 401, 'P01.8': 402, 'T43.505D': 403, 'N52.37': 404, 'N99.841': 405, 'T80.29XA': 406, 'T50.8X5D': 407, 'T43.025A': 408, 'T84.114A': 409, 'T80.818A': 410, 'N99.0': 411, 'O90.2': 412, 'P14.1': 413, 'T83.420A': 414, 'O74.8': 415, 'T84.040A': 416, 'N99.83': 417, 'I97.611': 418, 'T83.728S': 419, 'H21.81': 420, 'P12.89': 421, 'T83.6XXA': 422, 'P36.2': 423, 'Y84.5': 424, 'T45.7X5A': 425, 'M96.69': 426, 'P03.810': 427, 'T41.3X5A': 428, 'E89.3': 429, 'T84.031A': 430, 'O91.12': 431, 'E89.6': 432, 'T82.03XD': 433}\n"]}]},{"cell_type":"code","source":["class2label.int2str(256)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"jAJlkwDSFSKY","executionInfo":{"status":"ok","timestamp":1679507256222,"user_tz":0,"elapsed":6,"user":{"displayName":"Alexis Domínguez","userId":"12545493388305103028"}},"outputId":"08a306e8-9f36-4d91-9258-e3c9b18830f2"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'T44.7X5A'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["## Preprocess data\n","\n","As models like BERT don't expect text as direct input, but rather `input_ids`, etc., we tokenize the text using the tokenizer. Here I'm using the `AutoTokenizer` API, which will automatically load the appropriate tokenizer based on the checkpoint on the hub."],"metadata":{"id":"ePcPhmt4XlUG"}},{"cell_type":"markdown","source":[],"metadata":{"id":"XY-jgD9uFmpC"}},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","from transformers import AutoModelForSequenceClassification, DataCollatorForLanguageModeling, DataCollatorWithPadding\n","from transformers import TrainingArguments, Trainer\n","import numpy as np\n","from datasets import load_metric\n","\n","tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)\n","\n","def preprocess_data(examples):\n","  # take a batch of texts\n","  text = examples[\"text\"]\n","  # encode them\n","  encoding = tokenizer(text, padding=\"max_length\", truncation=True)\n","  # add label\n","  encoding[\"labels\"] = examples[\"label\"]\n","  return encoding"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["1b3940d67db54f7d8dfc92de16334916","ab0a6341efc04c0b90f73925b2f7604f","cce9cf9fe5004f4da42c387e358e4bd4","447eb309c1a2495fb4c4eceff3bb8773","fba97a43877540a79c546543178d52e6","8a1457cc3c7a47baaad073bbe113880b","a2ff46e5a7654dec8fbc2a9d6f9e7ad5","c85b1aa55f59400091bd62bfc8e8339f","9b78af5842d4487bb98e6d6cb4667248","37db87fd979c420eae00e5023ef0dd3b","b58bb18ef6154ece9aee5047589464c4","642bebccb3ed4c3f9ebd026233b5f347","d982186bc96f48a5a9e4c8e3424260c1","5ababc13bb0e47c181884c260c4244f4","3e9c86e1e5af42888643067bac6eefcc","22675d723fdd4a67a43f2043b35cda7b","aafb86e2f7424728afd98845c3dfd2dd","ffc33a37391a41db8b675be1bcca9d71","2da7461a95ce4dc69bf41a8096495a6d","df4820a5221346f1923be5d30adf5075","f7a3e7dfdca54b1c923a255e3323663c","e7416cd1af2246619cdcdfd531750b5b","19e09c682df542d6bbcea52c83e9c125","653022c0abff4dadbba45b1090fa4aea","47ee2ad342f64039a877372f82a5f43e","574455d57c4847359d8ffd563ed78b62","2232b644e38b46519b5b53af8bc51de0","5ac669a74e7e46fda4925bbbf08c3ddf","18020a2b04664381ae0a9a6eed4a09db","8953c0170d0b41e9997f23541f588ed5","8fade4918a9648aab65543c2e4ae8002","1f436c2d235043cb809037c998f56ccb","86010862c6804b649928bdb1977caf01","c19c808accb44a90a4d56f046ca1dfdd","f96620f48af9455c8610b90866034af9","7700d216fc6b4217bfef4f6f0c663d31","be147dd1695549868ce4e6eb037ed000","c3bb1a4f986e41a3b7787610dbb23897","51e94cef2ea54fb1a1c77d8470ec4bb7","46d4509f679a48f382498ea8281eac3e","6bef93a1941d420e8223717128c96b79","f9e0f0547f214ed48409d2c77c71f8c4","df1af0bae30f482abc3315785297fe68","1720daab0b57419f8729dd74519d96b9"]},"id":"FgYEmR1RxDBg","executionInfo":{"status":"ok","timestamp":1679507267349,"user_tz":0,"elapsed":11132,"user":{"displayName":"Alexis Domínguez","userId":"12545493388305103028"}},"outputId":"6207fbee-aceb-4667-af7f-d351f2f84586"},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b3940d67db54f7d8dfc92de16334916"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"642bebccb3ed4c3f9ebd026233b5f347"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19e09c682df542d6bbcea52c83e9c125"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c19c808accb44a90a4d56f046ca1dfdd"}},"metadata":{}}]},{"cell_type":"code","source":["dataset['train'][0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BmMDnf_4G7l_","executionInfo":{"status":"ok","timestamp":1679507267350,"user_tz":0,"elapsed":16,"user":{"displayName":"Alexis Domínguez","userId":"12545493388305103028"}},"outputId":"4d56c9ae-fd79-4e1b-c9ea-2007f0564d24"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'acto': 26792043,\n"," 'label': 256,\n"," 'label_str': 'T44.7X5A',\n"," 'labels_str': ['T44.7X5A', 'I95.2'],\n"," 'informes': ['26792043-169143208.txt'],\n"," 'text': ' _CABECERA_XXXX_  \\nSexo :Mujer\\n\\tINFORME ALTA DE MEDICINA INTERNA\\n\\tNIF/ _DATA_XXXX_ \\n\\t _DATA_XXXX_ \\n\\t _DATA_XXXX_ \\n\\t _DATA_XXXX_ \\n\\t_NOMBRE_XXXX_  \\n\\tTeléfono:  _DATA_XXXX_ \\n\\tTeléfono Móvil:  _DATA_XXXX_ \\n\\tFecha ingreso:  _FECHA_XXXX_   _HORA_XXXX_ \\nFecha alta:        _FECHA_XXXX_ \\n\\t _DOC_XXXX_ \\n\\tTipo de Ingreso: Desde urgencias\\n\\tMotivo de Ingreso (GP) Síncope posible hipotensivo. Problema social\\nMotivo de Alta\\n _APELLIDO_XXXX_ \\nMujer de 88 años.\\nAntecedentes Personales\\nNo refiere alergias conocidas a medicamentos\\nHTA. Cardiopatía hipertensiva con HVI moderada valorada por cardiología en 2008. Estudio de isquemia MIBI-DIPI negativo.\\nDM-2 sin tratamiento farmacológico.\\nDL.\\nNeoplasia maligna de útero \\nCarcinoma basocelular.\\nHerpes zoster torácico en 2010.\\nLumbalgia crónica mecánica. Aplastamiento de L1.\\nSd depresivo.\\nDeterioro cognitivo leve. Valorada por Neurología.\\n*Ingreso en 2015 en MINT por Neumonía LSD e ITU. Presentó SCA durante el ingreso.\\n*Episodios de urgencias por episodios de desorientación.\\nANTECEDENTES QUIRÚRGICOS: Cataratas bilateral, 2 cesáreas, fisura anal hace 15 años. Absceso isquiorectal (2008), reintervenido en dos ocasiones por proceso fistuloso.\\nSITUACIÓN BASAL: Vive en domicilio sola, activa e independiente para las ABVD. Doble continente. Deterioro cognitivo leve. No disnea ni ortopnea. No edemas en MMII. Anda con bastón.\\nTRATAMIENTO HABITUAL: COROPRES 25 1-0-0. DONEPEZILO 10. METAMIZOL. OMEPRAZOL 20 1-0-0. SERTRALINA 5 1-0-0,5. TRAMADOL/PARACETAMOL.\\nEnfermedad Actual\\nMujer de 88 años que acude a urgencias tras ser encontrada por su hijo en el suelo a las 19.00. La paciente presentó síncope con amnesia  aunque afirma pródromos al síncope consistente en visión borrosa y sudoración y en relación con la adopción de la bipedestación. Múltiples caídas de  _FECHA_XXXX_  para aquí en el contexto de la bipedestación, estando en el mercado. Asegura que siempre se cae hacia atrás con TCE occipital. Nota que las piernas no le aguantan y claudican. No relajación de esfínteres ni mordedura de lengua. Escoriaciones en ambos codos, y hematomas en ambas rodillas, dolor costal y lumbar. No dolor torácico previo ni palpitaciones ni sd miccional, ni dolor abdominal ni diarrea.\\nExploración Física\\nUrgencias\\nTA: 157/66; FC: 52 lpm. 95% basal.\\nHematomas en brazo izquierdo y pierna izquierda antiguos y recientes. A la exploración de hombro izquierdo no existe patrón capsular ni evidencia de rotura. Dolor a la palpación, no dolor a la movilización pasiva del miembro. AP: MVC sin ruidos sobreañadidos. AC: Rítmica sin soplos. ABD: RHA+, dolor a la palpación difusa, no masas ni megalias, timpánica, no peritonismo. MMII: No edemas, pulsos pedios conservados. NRL: No focalidad neurológica, moviliza 4 miembros, no rigidez de nuca, pares craneales normales, funciones superiores conservadas.\\nExploraciones Complementarias\\n  _FECHA_XXXX_  ECG: P sinusal  51 lpm. Rítmico. Eje normal. PR normal . QRS estrecho. QTc. No bloqueos ni datos de crecimiento de cavidades. Sin alteraciones agudas de la repolarización.\\n  _FECHA_XXXX_  Rx Tórax: No se observan claros infiltrados ni consolidaciones. No derrame pleural. ICT en límites de la normalidad.\\n  _FECHA_XXXX_  TC  craneal: No  se  observan  signos  de  sangrado  reciente,  colecciones  ni  efecto  de  masa  intra  ni  extraaxial. Correcta  diferenciación  entre  sustancia  blanca  y  sustancia  gris  supra  e  infratentorialmente,  apreciando  lesiones  hipodensas  en  sustancia  blanca  periventricular  en  probable  relación  con  enfermedad  isquémica  de  pequeño  vaso. Prominencia  de  surcos  y  ventrículos  en  relación  con  atrofia  corticosubcortical  difusa. Cisternas  libres.  Línea  media  centrada.  Ateromatosis  calcificada  de  sifones  carotídeos  y  arterias  vertebrales. Hiperostosis  frontal  interna \\nConclusión: \\nNo  se  observan  cambios  significativos  con  respecto  a  TC  previa  del  19/01/2015.\\n _FECHA_XXXX_  – Hemograma: LEU: 9.29 10^3/µL (3.50-11.00); Neut: 6.65 10^3/µL (2.0-7.5); linfoc: 1.77 10^3/µL (1.0-4.5); Monoci: 0.60 10^3/µL (0.2-0.8); Eosino: 0.00 10^3/µL (0.0-0.5); Basófi: 0.05 10^3/µL (0.0-0.2); LUC: 0.22 10^3µL (< =0); %Neut: 71.60 % (40.0-75.0); %Linfo: 19.10 % (20.0-45.0); %Monoc: 6.40 % (2.0-10.0); %Eosin: 0.00 % (1.0-6.0); %Basóf: 0.60 % (< =2); %LUC: 2.30 % (< =4); RBC: 4.59 10^6/µL (3.50-5.80); Hemogl: 14.30 g/dL (12.0-15.0); HTCO: 47.10 % (36.0-43.0)\\nVCM: 102.50 fL (78.0-100.0); HCM: 31.20 pg (27.0-32.0); CHCM: 30.50 g/dL (31.5-34.5); RDW-CV: 13.10 % (11.6-14.0); HDW: 2.78 g/dL (2.20-3.20); Plaquetas: 148.00 10^3/µL (130-450) Coagulación:  AP%: 107.40 % (80.0-120.0); INRa: 1.01 (< =1); APTT: 30.24 seg (25.0-35.0); Fi-der: 527.20 mg/dL (150.0-400.0); DDi: 1339.00 ng/ml (< =500) Bioquímica:  GLU: 82.00 mg/dl (70-110); URE: 76.00 mg/dl (10-50); CRE: 0.83 mg/dl (0.50-1.10); ALB: 3.20 g/dl (3.5-5.2); CA: 9.40 mg/dl (8.5-10.5); CAc: 10.3 mg/dl (8.6-10.2); NA: 139.00 mmol/L (135-147); K: 3.80 mmol/L (3.5-5.0); CL: 103.00 mmol/L (95-106); BT: 0.50 mg/dl (0.2-1.0); MIOG: 195.00 ng/ml (19.0-51.0); CK: 146.00 U/L (< =190); TNI: < 0.02 ng/ml (< =0); LDH: 288.00 U/L (80-235); GPT: 27.00 U/L (< =41); GOT: 28.00 U/L (< =31); FAL: 150.00 U/L (35-104) AMI: 29.00 U/L (< =100)\\n _FECHA_XXXX_  MN  Perfusión  pulmonar : Se  ha  realizado  gammagrafía  pulmonar,  despues  de  la  administración  por  vía  intravenosa  del  radiotrazador,  obteniendo  imágenes  planares,  en  proyecciones  anterior,  posterior,  laterales  y  oblicuas.  \\nPerfusión  pulmonar  homogénea  en  todos  los  campos,  sin  defectos  localizados  que  sugieran  TEP  en  la  actualidad. \\nResumen: Gammagrafía  pulmonar  sin  hallazgos  patológicos  significativos.  Se  descarta  razonablemente  la  existencia  de  TEP  en  la  actualidad. \\n11.09.2017Ecografía  Doppler  troncos  supraaórticos \\nTerritorio  carotídeo  DERECHO  de  características  morfológicas  normales.    No  hay  evidencia  de  ateromatosis  significativa.    Las  velocidades  y  curvas  espectrales  de  la  ACC,  ACI  y  ACE  están  dentro  de  límites  normales.    Se  detecta  flujo  en  la  arteria  vertebral  en  sentido  fisiológico.  Ratio  ACI/ACC:    0.64 \\nTerritorio  carotídeo  IZQUIERDO  de  características  morfológicas  normales.    No  hay  evidencia  de  ateromatosis  significativa.    Las  velocidades  y  curvas  espectrales  de  la  ACC,  ACI  y  ACE  están  dentro  de  límites  normales.    Se  detecta  flujo  en  la  arteria  vertebral  en  sentido  fisiológico.  Ratio  ACI/ACC:  1.32 \\nCONCLUSIÓN:  Sin  hallazgos  patológicos.\\n11/09/17-RX HOMBRO AP Y AXIAL: Sin alteraciones en marco óseo.\\n11/09/17Holter ECG: Ritmo sinusal con Fc media de 70, mínima de 48 y máxima de 98 lpm. 2 Extrasistoles ventriculares de 2 morfologías. 37 Extrasistoles supraventriculares. Asintomática durante el registro electrocardiográfico.\\n14-09-17-ESTUDIO EEG DE VIGILIA: Actividad de fondo simétrica, con ritmo alfa parieto occipital a 8 Hz. Ritmos rápidos de distribución fronto rolándica. \\nInterconsultas\\nAsuntos Sociales\\nMujer de 88 años de edad, viuda hace 20 años.Vive sola.\\nHijo único  _LOCALIDAD_XXXX_  móvil de contacto  _DATA_XXXX_  y nuera  _DATA_XXXX_ .\\nTramitada solicitud de reconocimiento de la ley de dependencia desestimada en  _FECHA_XXXX_  de 2014.Servicios sociales El  _NOMBRE_XXXX_ .\\nCoordinación para nueva cita e iniciar tramites de nuevo cita  _FECHA_XXXX_   a las  _HORA_XXXX_  horas.Entrego solicitud.\\nEntrego listado de empresas de asesoramiento de recursos y servicio de ayuda a domicilio.   \\nComentarios\\nMujer de 88 años que acude por episodio de pérdida de conciencia con pródromos previo, no observada por terceras personas, en paciente con betabloqueo como tratamiento antihipertensivo.  No datos indirectos de crisis tónico clónica (no pérdida control esfínteres o mordedura de lengua). No refiere disnea, palpitaciones o dolor torácico previo. Estos episodios han ocurrido en varias ocasiones previamente al episodio actual aunque no había consultado por ello. En urgencias no se objetivan taquicardias ni hipotensión, el EKG muestra un ritmo sinusal a 51 lpm, sin alteraciones del PR, QRS ni intervalo QT. Tampoco alteraciones destacables en placa de torax. No existen soplos sugerentes de estenosis aórtica severa y tiene un ecocardio previo sin valvulopatías. El TAC craneal no revela alteraciones significativas. Los marcadores enzimáticos cardíacos son normales. Gammagrafía de perfusión que descarta TEP. Se ingresa para estudio con retirada de betabloqueantes. En planta se completa el estudio con Doppler de TSA que no muestra alteraciones, un Holter de EKG sin alteraciones que justifiquen sincope (RS con frecuencia minima nocturna de 48 y máxima diurna de 98, con 2 extrasístoles ventriculares y 37 extrasístoles supraventriculares no asociados a evento clínico) y un electroencefalografía sin datos de actividad. Durante su ingreso no ha presentado clínica de mareo o síncopes y ha comenzado a caminar sin aparición de síntomas. Se concluye como hipótesis de los sincopes el tratamiento con betabloqueantes por lo que se suspenden. Las tensiones durante el ingreso han oscilado desde sistólicas de 125 o sistólicas de 155, por lo que se añade al tratamiento iecas a bajas dosis (enalapril 5 mg al día). Control clínico y de tensión arterial por su médico de primaria. Por otra parte, se evidenció en urgencias un sedimento de orina patológico con urocultivo postivo para E.Colli, tratándose la infección de orina con ciprofloxacino oral.\\nLa paciente tiene una Marcha de precaución por su edad que podría aumentar el riesgo de caídas por lo que se recomienda marcha con bastón, u andador, evitar barreras físicas en domicilio y supervivisión lo que se pone en conocimiento de la familia.\\nJuicio Clínico\\n-Síncope posiblemente hipotensivo y con bradicardia por toma de carvedilol. \\nDesacondicionamiento funcional progresivo.\\n-ITU por E. coli.\\nLos previos: HTA, Cardiopatía Hipertensiva, Deterioro cognitivo leve, Sd depresivo.\\nTratamiento\\nAÑADIRÁ A SU TRATAMIENTO PREVIO\\n· ENALAPRIL 5 mg: 1-0-0 (un comprimido en el desayuno).\\nSUSPENDERÁ DE SU TRATAMIENTO PREVIO\\n· COROPRES 25 1-0-0. \\nCONTINUARÁ CON EL RESTO DE SU TRATAMIENTO PREVIO\\n· DONEPEZILO 10. METAMIZOL (SI PRECISA). OMEPRAZOL 20 1-0-0. SERTRALINA 50mg: 1-0-0,5. TRAMADOL/PARACETAMOL (SI PRECISA)\\nRecomendaciones\\n\\tEn cumplimiento del artículo 5 de la LOPD  _FECHA_XXXX_ , se informa que sus datos identificativos y de salud serán objeto de tratamiento e incorporados a los ficheros de datos sanitarios, cuya titularidad corresponde al  _HOSPITAL_XXXX_ . Los datos únicamente serán utilizados con fines asociados a la atención y gestión sanitaria, docencia y seguimiento asistencial, estando prevista su comunicación a los organismos públicos con competencia en materia sanitaria. \\nEl órgano ante el que podrá ejercer los derechos de acceso, cancelación, rectificación y oposición de datos es el Servicio de Atención al Usuario del  _HOSPITAL_XXXX_ ,   _DIRECCION_XXXX_ \\n'}"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["preprocess_data(dataset['train'][0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-ApAkTTlGo_z","executionInfo":{"status":"ok","timestamp":1679507267350,"user_tz":0,"elapsed":15,"user":{"displayName":"Alexis Domínguez","userId":"12545493388305103028"}},"outputId":"b5d5a1ac-6f8b-4c3d-f607-006c5ef36828"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': [0, 18134, 347, 4546, 3586, 32830, 1215, 48193, 1215, 1437, 1437, 50118, 35581, 139, 4832, 448, 11591, 254, 50118, 50117, 2444, 27201, 6570, 6019, 3847, 5885, 22718, 2371, 16712, 20281, 4444, 50118, 50117, 487, 7025, 73, 18134, 48242, 1215, 48193, 1215, 1437, 50118, 50117, 18134, 48242, 1215, 48193, 1215, 1437, 50118, 50117, 18134, 48242, 1215, 48193, 1215, 1437, 50118, 50117, 18134, 48242, 1215, 48193, 1215, 1437, 50118, 50117, 1215, 487, 3765, 37589, 1215, 48193, 1215, 1437, 1437, 50118, 50117, 36417, 1140, 506, 16405, 35, 1437, 18134, 48242, 1215, 48193, 1215, 1437, 50118, 50117, 36417, 1140, 506, 16405, 256, 1479, 16756, 35, 1437, 18134, 48242, 1215, 48193, 1215, 1437, 50118, 50117, 597, 7529, 102, 21691, 1535, 139, 35, 1437, 18134, 597, 3586, 6826, 1215, 48193, 1215, 1437, 1437, 18134, 725, 3411, 250, 1215, 48193, 1215, 1437, 50118, 597, 7529, 102, 11838, 102, 35, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 18134, 597, 3586, 6826, 1215, 48193, 1215, 1437, 50118, 50117, 18134, 46570, 1215, 48193, 1215, 1437, 50118, 50117, 43691, 139, 263, 11996, 1535, 139, 35, 4762, 2794, 11540, 4138, 2520, 281, 50118, 50117, 41676, 9697, 263, 11996, 1535, 139, 36, 12694, 43, 208, 1977, 11326, 9877, 8593, 4748, 6605, 1242, 1290, 9697, 4, 1698, 5225, 1916, 592, 50118, 41676, 9697, 263, 726, 4349, 50118, 18134, 591, 25322, 2688, 673, 1215, 48193, 1215, 1437, 50118, 448, 11591, 254, 263, 7953, 10, 6303, 366, 4, 50118, 18348, 3204, 196, 1342, 293, 18404, 4575, 50118, 3084, 4885, 14116, 1076, 11249, 5003, 2764, 1975, 19926, 10, 26467, 5511, 366, 50118, 725, 3847, 4, 5866, 28119, 415, 5272, 1368, 17453, 90, 1290, 7222, 2764, 289, 15176, 23758, 2095, 7398, 368, 2095, 2953, 1886, 118, 8982, 5272, 1177, 2266, 4, 5441, 1906, 1020, 263, 16, 2253, 23249, 10931, 5383, 12, 495, 3808, 100, 15183, 415, 9697, 4, 50118, 25652, 12, 176, 10272, 2664, 415, 424, 4843, 139, 3380, 1043, 1168, 1479, 571, 2684, 4, 50118, 26109, 4, 50118, 14563, 25934, 19036, 8196, 4932, 102, 263, 952, 3070, 1334, 139, 1437, 50118, 347, 9636, 179, 4982, 11909, 139, 9736, 8244, 4, 50118, 13584, 29270, 992, 13991, 16535, 1526, 438, 2684, 1177, 1824, 4, 50118, 574, 4179, 39883, 3977, 2727, 2426, 162, 438, 7499, 2426, 4, 83, 2911, 1988, 424, 4843, 139, 263, 226, 134, 4, 50118, 104, 417, 8273, 1535, 9697, 4, 50118, 495, 5906, 7375, 139, 28105, 405, 9697, 16066, 4, 3767, 368, 2095, 2953, 39053, 5272, 4, 50118, 3226, 41702, 1535, 139, 1177, 570, 1177, 256, 17831, 2953, 3864, 783, 261, 5272, 37619, 364, 3779, 791, 4, 17356, 1479, 4998, 250, 17373, 5285, 1615, 21691, 1535, 139, 4, 50118, 3226, 39318, 354, 1630, 4544, 263, 11540, 4138, 2520, 281, 2953, 38862, 1630, 4544, 263, 2694, 29458, 5014, 2727, 4, 50118, 11088, 3586, 1691, 5382, 1723, 16944, 5216, 3849, 15113, 41626, 2371, 3196, 35, 7641, 15021, 281, 9526, 6, 132, 740, 293, 1526, 241, 281, 6, 856, 354, 3851, 26363, 1368, 4450, 379, 10, 6303, 366, 4, 23709, 4643, 139, 16, 2253, 118, 1688, 3894, 337, 36, 27418, 238, 769, 8007, 2987, 6005, 1177, 9958, 1021, 438, 27720, 293, 2953, 1759, 4643, 139, 19033, 922, 18865, 4, 50118, 104, 2068, 791, 2562, 100, 3849, 9085, 487, 17430, 2118, 35, 23951, 1177, 13567, 636, 718, 1020, 9281, 102, 6, 30264, 102, 364, 44913, 4843, 242, 3840, 5573, 6266, 28732, 4, 16149, 459, 9183, 242, 4, 40344, 7375, 139, 28105, 405, 9697, 16066, 4, 440, 2982, 22423, 10265, 50, 8766, 22423, 4, 440, 4803, 991, 281, 1177, 24537, 11194, 4, 178, 102, 2764, 25753, 2727, 4, 50118, 6997, 2571, 2620, 44735, 673, 289, 4546, 2068, 17710, 35, 19328, 5733, 32030, 564, 112, 12, 288, 12, 288, 4, 24980, 9662, 717, 1301, 3063, 673, 158, 4, 30782, 2620, 17045, 3384, 4, 23765, 9662, 4396, 1301, 3384, 291, 112, 12, 288, 12, 288, 4, 28408, 6997, 2118, 16712, 195, 112, 12, 288, 12, 288, 6, 245, 4, 5758, 2620, 2606, 3384, 73, 14280, 2562, 3935, 2620, 3384, 4, 50118, 16040, 6646, 4567, 625, 30144, 50118, 448, 11591, 254, 263, 7953, 10, 6303, 366, 1192, 4285, 6343, 10, 11540, 4138, 2520, 281, 2664, 281, 6821, 9689, 2533, 338, 2095, 2953, 2628, 16195, 139, 1177, 1615, 2628, 13961, 10, 5573, 753, 4, 612, 4, 1587, 18234, 4843, 242, 1455, 1479, 579, 1977, 11326, 9877, 2764, 524, 40324, 1437, 10, 879, 3407, 9724, 9856, 102, 3349, 1479, 417, 5638, 366, 1076, 579, 1977, 11326, 9877, 4292, 242, 1177, 17737, 118, 2727, 741, 368, 3985, 102, 1423, 2628, 32649, 5014, 2727, 1423, 1177, 6258, 5014, 2727, 2764, 897, 25298, 2520, 2727, 263, 897, 38706, 196, 990, 5014, 2727, 4, 256, 5874, 7984, 118, 12349, 6056, 1977, 417, 281, 263, 1437, 18134, 597, 3586, 6826, 1215, 48193, 1215, 1437, 3840, 16690, 1977, 1177, 1615, 5377, 139, 263, 897, 38706, 196, 990, 5014, 2727, 6, 3304, 5502, 1177, 1615, 22415, 2102, 4, 83, 1090, 571, 3851, 1192, 579, 9173, 5234, 842, 6056, 242, 1368, 21647, 23, 338, 6417, 2764, 255, 8041, 37627, 1588, 8632, 4, 1491, 102, 1192, 5573, 20943, 22118, 117, 2084, 5951, 257, 927, 260, 1423, 3741, 5247, 12657, 4, 440, 6258, 1176, 5014, 2727, 263, 2714, 506, 1977, 3999, 242, 1535, 10265, 475, 3109, 196, 3851, 263, 34583, 4324, 4, 11274, 368, 9504, 1499, 293, 1177, 13569, 366, 20993, 366, 6, 1423, 37, 9244, 27745, 1177, 13569, 281, 21966, 19485, 6, 385, 37194, 701, 337, 1423, 784, 4179, 271, 4, 440, 385, 37194, 16535, 1526, 438, 2684, 21720, 1020, 10265, 8750, 17291, 8647, 293, 10265, 46409, 14926, 438, 6073, 6, 10265, 385, 37194, 28670, 10265, 2269, 19205, 102, 4, 50118, 43043, 368, 5014, 2727, 274, 18890, 2426, 50118, 38046, 4138, 2520, 281, 50118, 3847, 35, 26289, 73, 4280, 131, 5429, 35, 3135, 784, 1685, 4, 6164, 207, 45753, 4, 50118, 725, 991, 415, 27745, 1177, 741, 9919, 139, 1437, 1210, 2253, 906, 5016, 1423, 20943, 2133, 1437, 1210, 2253, 906, 6106, 9876, 1023, 257, 366, 1423, 3872, 4843, 293, 4, 83, 897, 30262, 5014, 2727, 263, 1368, 5223, 1001, 1437, 1210, 2253, 906, 5016, 117, 5152, 242, 10512, 338, 2727, 9686, 8244, 10265, 42925, 16446, 263, 13351, 3851, 4, 13520, 368, 10, 897, 46941, 5014, 2727, 6, 117, 385, 37194, 10, 897, 32924, 718, 1210, 5014, 2727, 6977, 7222, 2424, 11163, 991, 7450, 4, 1480, 35, 256, 14858, 10272, 6365, 808, 366, 98, 7805, 102, 6303, 625, 808, 366, 4, 7224, 35, 248, 1977, 26989, 2426, 10272, 579, 25934, 366, 4, 6266, 495, 35, 248, 6826, 30787, 385, 37194, 10, 897, 46941, 5014, 2727, 385, 1594, 13253, 6, 117, 11705, 281, 10265, 162, 9487, 5003, 6, 15679, 642, 7499, 2426, 6, 117, 228, 24899, 19689, 4, 24537, 11194, 35, 440, 4803, 991, 281, 6, 33463, 366, 16954, 4544, 23923, 6510, 4, 18936, 35, 440, 25284, 29849, 625, 14913, 462, 1479, 571, 2426, 6, 32924, 718, 10071, 204, 11163, 20506, 3985, 6, 117, 10727, 1949, 329, 263, 295, 26802, 6, 181, 5347, 28472, 4575, 13071, 4575, 6, 26437, 1499, 293, 2422, 118, 4765, 23923, 13920, 4, 50118, 43043, 368, 8647, 293, 4556, 40224, 1512, 281, 50118, 1437, 18134, 597, 3586, 6826, 1215, 48193, 1215, 1437, 11270, 534, 35, 221, 10272, 25016, 1437, 4074, 784, 1685, 4, 248, 1977, 26989, 2684, 4, 381, 2359, 2340, 4, 4729, 2340, 479, 1209, 8105, 3304, 241, 11156, 4, 1209, 565, 438, 4, 440, 20869, 3407, 366, 10265, 13516, 366, 263, 8633, 438, 757, 4843, 139, 263, 740, 23656, 4216, 4, 8356, 11330, 8647, 293, 5951, 1906, 281, 263, 897, 2851, 19231, 1210, 5014, 2727, 4, 50118, 1437, 18134, 597, 3586, 6826, 1215, 48193, 1215, 1437, 44681, 255, 1479, 35054, 35, 440, 842, 20717, 260, 17691, 366, 39636, 6510, 10265, 24707, 8647, 293, 4, 440, 1935, 38007, 16415, 9799, 4, 38, 7164, 1177, 784, 1977, 119, 5110, 263, 897, 2340, 8843, 4, 50118, 1437, 18134, 597, 3586, 6826, 1215, 48193, 1215, 1437, 21137, 1437, 28472, 337, 35, 440, 1437, 842, 1437, 20717, 260, 1437, 1203, 366, 1437, 263, 1437, 11944, 338, 2102, 1437, 3872, 4843, 242, 6, 1437, 1029, 459, 7309, 1499, 293, 1437, 10265, 1437, 364, 17706, 139, 1437, 263, 1437, 11705, 102, 1437, 18592, 1437, 10265, 1437, 1823, 3631, 2617, 4, 37234, 102, 1437, 385, 1594, 8663, 2520, 5014, 2727, 1437, 3838, 241, 1437, 29237, 3290, 493, 1437, 3089, 19740, 1437, 1423, 1437, 29237, 3290, 493, 1437, 4435, 354, 1437, 46382, 1437, 364, 1437, 4047, 6528, 1342, 17707, 1757, 242, 6, 1437, 6256, 19954, 5502, 1437, 7427, 1499, 293, 1437, 6605, 1630, 1290, 281, 1437, 1177, 1437, 29237, 3290, 493, 1437, 3089, 19740, 1437, 228, 1879, 44581, 8244, 1437, 1177, 1437, 15186, 1437, 6258, 5014, 2727, 1437, 2764, 1437, 1177, 6646, 4567, 625, 1437, 16, 2253, 1140, 119, 2426, 1437, 263, 1437, 3723, 3407, 14182, 1437, 37345, 139, 4, 10772, 179, 16446, 1437, 263, 1437, 8113, 16254, 1437, 1423, 1437, 13228, 338, 1977, 13300, 366, 1437, 1177, 1437, 6258, 5014, 2727, 1437, 2764, 1437, 35790, 506, 493, 1437, 30249, 636, 366, 1792, 438, 2723, 3569, 1437, 385, 1594, 13253, 4, 230, 354, 18995, 281, 1437, 21748, 1535, 4, 1437, 226, 1977, 22423, 1437, 433, 1437, 41248, 2095, 4, 1437, 83, 1334, 1075, 415, 13310, 1437, 13011, 45404, 2095, 1437, 263, 1437, 579, 1594, 6909, 1437, 512, 1242, 1977, 2794, 366, 1437, 1423, 1437, 39929, 5003, 1437, 32969, 44283, 293, 4, 289, 17453, 2603, 13310, 1437, 39102, 1437, 11587, 102, 1437, 50118, 9157, 19734, 118, 2727, 35, 1437, 50118, 3084, 1437, 842, 1437, 20717, 260, 1437, 740, 3146, 4544, 1437, 40042, 415, 1879, 366, 1437, 2764, 1437, 2098, 139, 1437, 10, 1437, 21137, 1437, 21720, 493, 1437, 2424, 1437, 753, 73, 2663, 73, 14420, 4, 50118, 18134, 597, 3586, 6826, 1215, 48193, 1215, 1437, 126, 10869, 2154, 24016, 35, 10611, 791, 35, 361, 4, 2890, 158, 35227, 246, 73, 4056, 8906, 574, 36, 246, 4, 1096, 12, 1225, 4, 612, 4397, 3864, 1182, 35, 231, 4, 3506, 158, 35227, 246, 73, 4056, 8906, 574, 36, 176, 4, 288, 12, 406, 4, 245, 4397, 24248, 506, 1975, 35, 112, 4, 4718, 158, 35227, 246, 73, 4056, 8906, 574, 36, 134, 4, 288, 12, 306, 4, 245, 4397, 3385, 17641, 35, 321, 4, 2466, 158, 35227, 246, 73, 4056, 8906, 574, 36, 288, 4, 176, 12, 288, 4, 398, 4397, 381, 366, 1696, 35, 321, 4, 612, 158, 35227, 246, 73, 4056, 8906, 574, 36, 288, 4, 288, 12, 288, 4, 245, 4397, 7093, 1479, 9169, 35, 321, 4, 2546, 158, 35227, 246, 73, 4056, 8906, 574, 36, 288, 4, 288, 12, 288, 4, 176, 4397, 226, 12945, 35, 321, 4, 2036, 158, 35227, 246, 4056, 8906, 574, 48082, 5457, 288, 4397, 7606, 14563, 1182, 35, 6121, 4, 2466, 7606, 36, 1749, 4, 288, 12, 2545, 4, 288, 4397, 7606, 574, 23999, 35, 753, 4, 698, 7606, 36, 844, 4, 288, 12, 1898, 4, 288, 4397, 7606, 17312, 1975, 35, 231, 4, 1749, 7606, 36, 176, 4, 288, 12, 698, 4, 288, 4397, 7606, 717, 366, 179, 35, 321, 4, 612, 7606, 36, 134, 4, 288, 12, 401, 4, 288, 4397, 7606, 40258, 1479, 506, 35, 321, 4, 2466, 7606, 48082, 5457, 176, 4397, 7606, 574, 12945, 35, 132, 4, 541, 7606, 48082, 5457, 306, 4397, 248, 3573, 35, 204, 4, 4156, 158, 35227, 401, 73, 4056, 8906, 574, 36, 246, 4, 1096, 12, 245, 4, 2940, 4397, 10869, 25326, 35, 501, 4, 541, 821, 73, 45921, 36, 1092, 4, 288, 12, 996, 4, 288, 4397, 16484, 673, 35, 4034, 4, 698, 7606, 36, 3367, 4, 288, 12, 3897, 4, 288, 43, 50118, 14858, 448, 35, 12747, 4, 1096, 856, 574, 36, 5479, 4, 288, 12, 1866, 4, 288, 4397, 289, 18814, 35, 1105, 4, 844, 47194, 36, 2518, 4, 288, 12, 2881, 4, 288, 4397, 3858, 18814, 35, 389, 4, 1096, 821, 73, 45921, 36, 2983, 4, 245, 12, 3079, 4, 245, 4397, 30198, 771, 12, 35433, 35, 508, 4, 698, 7606, 36, 1225, 4, 401, 12, 1570, 4, 288, 4397, 7951, 771, 35, 132, 4, 5479, 821, 73, 45921, 36, 176, 4, 844, 12, 246, 4, 844, 4397, 3037, 102, 15259, 281, 35, 24646, 4, 612, 158, 35227, 246, 73, 4056, 8906, 574, 36, 11343, 12, 13872, 43, 944, 1073, 922, 5014, 2727, 35, 1437, 1480, 45737, 13674, 4, 1749, 7606, 36, 2940, 4, 288, 12, 10213, 4, 288, 4397, 2808, 27625, 35, 112, 4, 2663, 48082, 5457, 134, 4397, 1480, 14543, 35, 389, 4, 1978, 842, 571, 36, 1244, 4, 288, 12, 2022, 4, 288, 4397, 19643, 12, 3624, 35, 195, 2518, 4, 844, 17844, 73, 45921, 36, 6115, 4, 288, 12, 4017, 4, 288, 4397, 211, 29038, 35, 508, 3416, 4, 612, 6094, 73, 18517, 48082, 5457, 1497, 43, 12334, 2253, 1977, 119, 2426, 35, 1437, 12209, 791, 35, 7383, 4, 612, 17844, 73, 30469, 36, 3083, 12, 11670, 4397, 121, 4629, 35, 5553, 4, 612, 17844, 73, 30469, 36, 698, 12, 1096, 4397, 28122, 35, 321, 4, 6361, 17844, 73, 30469, 36, 288, 4, 1096, 12, 134, 4, 698, 4397, 6019, 387, 35, 155, 4, 844, 821, 73, 30469, 36, 246, 4, 245, 12, 245, 4, 176, 4397, 5267, 35, 361, 4, 1749, 17844, 73, 30469, 36, 398, 4, 245, 12, 698, 4, 245, 4397, 5267, 438, 35, 158, 4, 246, 17844, 73, 30469, 36, 398, 4, 401, 12, 698, 4, 176, 4397, 8438, 35, 24404, 4, 612, 48942, 73, 574, 36, 18508, 12, 26629, 4397, 229, 35, 155, 4, 2940, 48942, 73, 574, 36, 246, 4, 245, 12, 245, 4, 288, 4397, 5289, 35, 13156, 4, 612, 48942, 73, 574, 36, 4015, 12, 18427, 4397, 12482, 35, 321, 4, 1096, 17844, 73, 30469, 36, 288, 4, 176, 12, 134, 4, 288, 4397, 10931, 10207, 35, 22871, 4, 612, 6094, 73, 18517, 36, 1646, 4, 288, 12, 4708, 4, 288, 4397, 35281, 35, 24543, 4, 612, 121, 73, 574, 48082, 5457, 20109, 4397, 255, 17640, 35, 28696, 321, 4, 4197, 6094, 73, 18517, 48082, 5457, 288, 4397, 34744, 725, 35, 35395, 4, 612, 121, 73, 574, 36, 2940, 12, 23821, 4397, 272, 10311, 35, 974, 4, 612, 121, 73, 574, 48082, 5457, 4006, 4397, 42007, 35, 971, 4, 612, 121, 73, 574, 48082, 5457, 2983, 4397, 274, 2118, 35, 3982, 4, 612, 121, 73, 574, 36, 2022, 12, 17573, 43, 3326, 100, 35, 1132, 4, 612, 121, 73, 574, 48082, 5457, 1866, 43, 50118, 18134, 597, 3586, 6826, 1215, 48193, 1215, 1437, 18138, 1437, 2595, 506, 687, 118, 2727, 1437, 25578, 5806, 271, 4832, 1608, 1437, 2489, 1437, 588, 1210, 2102, 1437, 821, 12614, 1073, 13883, 5272, 1437, 25578, 5806, 271, 6, 1437, 18690, 3663, 1437, 263, 1437, 897, 1437, 32626, 5014, 2727, 1437, 2953, 1437, 748, 5272, 1437, 38553, 5166, 1437, 2424, 1437, 13206, 13214, 9919, 9618, 6, 1437, 6168, 3869, 118, 12454, 1437, 4356, 1526, 4138, 293, 1437, 563, 5347, 6, 1437, 1177, 1437, 1759, 4717, 7309, 1499, 293, 1437, 34988, 6, 1437, 41834, 6, 1437, 423, 4575, 1437, 1423, 1437, 6168, 5895, 257, 281, 4, 1437, 1437, 50118, 20823, 506, 687, 118, 2727, 1437, 25578, 5806, 271, 1437, 9486, 2154, 1140, 22423, 1437, 1177, 1437, 7, 19202, 1437, 3774, 1437, 2205, 366, 6, 1437, 10272, 1437, 17584, 366, 1437, 400, 1210, 6510, 1437, 1192, 1437, 2628, 571, 906, 260, 1437, 255, 9662, 1437, 1177, 1437, 897, 1437, 3031, 8843, 4, 1437, 50118, 20028, 18546, 35, 272, 12614, 1073, 13883, 5272, 1437, 25578, 5806, 271, 1437, 10272, 1437, 5179, 1222, 571, 366, 1437, 10512, 1168, 1479, 571, 636, 366, 1437, 40042, 415, 1879, 366, 4, 1437, 1608, 1437, 23174, 28721, 1437, 910, 20524, 868, 1757, 242, 1437, 897, 1437, 5152, 16446, 1437, 263, 1437, 255, 9662, 1437, 1177, 1437, 897, 1437, 3031, 8843, 4, 1437, 50118, 1225, 4, 3546, 4, 3789, 44220, 2154, 13883, 5272, 1437, 1832, 3807, 1371, 1437, 2664, 261, 16254, 1437, 46382, 102, 1479, 338, 13240, 366, 1437, 50118, 32579, 3961, 18324, 1437, 512, 1242, 1977, 2794, 139, 1437, 211, 2076, 32958, 673, 1437, 263, 1437, 512, 35995, 1977, 29048, 281, 1437, 14628, 22588, 1479, 571, 31895, 1437, 13071, 4575, 4, 1437, 1437, 1437, 440, 1437, 13460, 1437, 42925, 16446, 1437, 263, 1437, 10, 1334, 1075, 415, 13310, 1437, 40042, 415, 7222, 4, 1437, 1437, 1437, 2588, 1437, 23021, 1975, 808, 4216, 1437, 1423, 1437, 5350, 20670, 1437, 2714, 13771, 7085, 293, 1437, 263, 1437, 897, 1437, 10018, 6, 1437, 7224, 100, 1437, 1423, 1437, 36211, 1437, 3304, 7499, 1437, 14368, 1001, 1437, 263, 1437, 784, 1977, 119, 5110, 1437, 13071, 4575, 4, 1437, 1437, 1437, 1608, 1437, 10933, 102, 1437, 6626, 3548, 1437, 1177, 1437, 897, 1437, 39929, 493, 1437, 32969, 44283, 1437, 1177, 1437, 1051, 6005, 1437, 856, 354, 14215, 1479, 571, 2684, 4, 1437, 20475, 1437, 7224, 100, 73, 21678, 35, 1437, 1437, 1437, 321, 4, 4027, 1437, 50118, 32579, 3961, 18324, 1437, 512, 1242, 1977, 2794, 139, 1437, 38, 1301, 48055, 2076, 19174, 1437, 263, 1437, 512, 35995, 1977, 29048, 281, 1437, 14628, 22588, 1479, 571, 31895, 1437, 13071, 4575, 4, 1437, 1437, 1437, 440, 1437, 13460, 1437, 42925, 16446, 1437, 263, 1437, 10, 1334, 1075, 415, 13310, 1437, 40042, 415, 7222, 4, 1437, 1437, 1437, 2588, 1437, 23021, 1975, 808, 4216, 1437, 1423, 1437, 5350, 20670, 1437, 2714, 13771, 7085, 293, 1437, 263, 1437, 897, 1437, 10018, 6, 1437, 7224, 100, 1437, 1423, 1437, 36211, 1437, 3304, 7499, 1437, 14368, 1001, 1437, 263, 1437, 784, 1977, 119, 5110, 1437, 13071, 4575, 4, 1437, 1437, 1437, 1608, 1437, 10933, 102, 1437, 6626, 3548, 1437, 1177, 1437, 897, 1437, 39929, 493, 1437, 32969, 44283, 1437, 1177, 1437, 1051, 6005, 1437, 856, 354, 14215, 1479, 571, 2684, 4, 1437, 20475, 1437, 7224, 100, 73, 21678, 35, 1437, 112, 4, 2881, 1437, 50118, 49760, 100, 3849, 9085, 487, 35, 1437, 8356, 1437, 5179, 1222, 571, 366, 1437, 10512, 1168, 1479, 571, 636, 366, 4, 50118, 1225, 73, 3546, 73, 1360, 12, 500, 1000, 289, 3765, 7202, 673, 1480, 854, 31697, 11694, 35, 8356, 11330, 8647, 293, 1177, 4401, 876, 952, 15264, 1090, 139, 4, 50118, 1225, 73, 3546, 73, 1360, 32139, 1334, 11270, 534, 35, 248, 405, 4992, 10272, 25016, 2764, 274, 438, 433, 263, 1510, 6, 475, 17010, 4261, 263, 2929, 1423, 475, 1526, 1178, 4261, 263, 8757, 784, 1685, 4, 132, 44268, 661, 7003, 13228, 4063, 922, 5347, 263, 132, 14628, 506, 8982, 1977, 281, 4, 2908, 44268, 661, 7003, 46382, 9399, 4063, 922, 5347, 4, 287, 2544, 1075, 1526, 90, 2426, 17373, 5285, 1615, 19886, 1001, 29541, 6940, 118, 2154, 338, 1526, 506, 2684, 4, 50118, 1570, 12, 3546, 12, 1360, 12, 4923, 13083, 6454, 46023, 5885, 468, 5969, 3063, 2889, 35, 1783, 6837, 625, 263, 10882, 139, 16207, 10221, 14962, 6, 2764, 910, 405, 4992, 1076, 12010, 2242, 5810, 139, 37627, 1588, 8632, 10, 290, 46265, 4, 248, 405, 14666, 910, 1526, 43708, 366, 263, 7018, 8643, 20296, 2727, 760, 139, 4533, 462, 1526, 1187, 2426, 4, 1437, 50118, 26267, 10998, 6070, 281, 50118, 1620, 5973, 366, 3574, 293, 50118, 448, 11591, 254, 263, 7953, 10, 6303, 366, 263, 4803, 625, 6, 12987, 10876, 1368, 4450, 291, 10, 6303, 366, 4, 846, 2088, 9281, 102, 4, 50118, 725, 2161, 139, 952, 3070, 282, 2684, 1437, 18134, 45002, 2118, 2688, 2606, 1215, 48193, 1215, 1437, 475, 1479, 16756, 263, 1511, 139, 1437, 18134, 48242, 1215, 48193, 1215, 1437, 1423, 6505, 3843, 1437, 18134, 48242, 1215, 48193, 1215, 479, 50118, 565, 4040, 405, 2095, 22706, 1906, 263, 16689, 1975, 757, 4843, 139, 263, 897, 2084, 219, 263, 6723, 16446, 2694, 990, 757, 2095, 1177, 1437, 18134, 597, 3586, 6826, 1215, 48193, 1215, 1437, 263, 777, 4, 44175, 636, 4544, 592, 293, 1448, 1437, 18134, 487, 3765, 37589, 1215, 48193, 1215, 479, 50118, 8739, 3109, 179, 5014, 2727, 3840, 295, 1780, 3952, 740, 3119, 364, 11, 636, 12202, 24599, 5110, 263, 295, 1780, 5766, 740, 3119, 1437, 18134, 597, 3586, 6826, 1215, 48193, 1215, 1437, 1437, 10, 5573, 1437, 18134, 725, 3411, 250, 1215, 48193, 1215, 1437, 17211, 281, 4, 30495, 241, 2977, 22706, 1906, 4, 50118, 30495, 241, 2977, 889, 2102, 263, 2841, 13529, 281, 263, 25, 293, 368, 424, 4843, 139, 263, 3872, 4668, 366, 1423, 18542, 1020, 263, 16749, 10876, 10, 13567, 636, 718, 1020, 4, 1437, 1437, 1437, 50118, 14721, 1342, 24198, 50118, 448, 11591, 254, 263, 7953, 10, 6303, 366, 1192, 4285, 6343, 2953, 38862, 1630, 1020, 263, 181, 1140, 2586, 4347, 263, 2764, 2520, 16446, 2764, 3349, 1479, 417, 5638, 366, 21720, 1020, 6, 117, 20717, 2095, 2953, 8470, 7742, 281, 621, 281, 6, 1177, 18234, 4843, 242, 2764, 5673, 42180, 3407, 139, 3137, 139, 2664, 415, 424, 4843, 139, 1475, 3592, 11497, 1290, 9697, 4, 1437, 440, 13516, 366, 18677, 366, 263, 1486, 326, 2727, 2684, 3741, 2727, 2426, 36, 2362, 181, 1140, 2586, 4347, 797, 2714, 506, 1977, 3999, 242, 1535, 1021, 475, 3109, 196, 3851, 263, 34583, 4324, 322, 440, 4885, 14116, 2982, 22423, 6, 8750, 17291, 8647, 293, 1021, 385, 37194, 16535, 1526, 438, 2684, 21720, 1020, 4, 5441, 366, 38862, 1630, 4544, 1368, 260, 1021, 17742, 10505, 139, 1177, 15747, 5003, 1021, 438, 27720, 293, 21720, 6009, 8530, 1076, 38862, 1630, 1020, 3031, 10, 879, 3407, 117, 2489, 428, 5272, 12777, 2102, 2953, 364, 9905, 4, 2271, 11540, 4138, 2520, 281, 117, 842, 26907, 594, 18356, 6735, 2253, 636, 1120, 5003, 10265, 6605, 1242, 1290, 118, 2727, 6, 1615, 381, 530, 534, 14701, 990, 763, 542, 910, 405, 4992, 10272, 25016, 10, 4074, 784, 1685, 6, 10272, 11330, 8647, 293, 2424, 4729, 6, 1209, 8105, 10265, 22455, 139, 1209, 565, 4, 255, 3914, 11959, 11330, 8647, 293, 15357, 1043, 6058, 1177, 15155, 102, 263, 16535, 3631, 4, 440, 5152, 225, 579, 25934, 366, 2628, 1899, 9854, 293, 263, 3304, 225, 13310, 10, 1479, 9713, 2426, 20242, 102, 1423, 12369, 2552, 542, 20508, 43682, 1020, 21720, 1020, 10272, 7398, 705, 922, 1517, 415, 1977, 281, 4, 1448, 255, 2562, 28472, 337, 117, 20853, 102, 11330, 8647, 293, 40042, 415, 20559, 4, 1287, 4401, 438, 625, 4765, 46538, 757, 1526, 13240, 366, 1886, 1977, 1043, 366, 979, 13071, 4575, 4, 272, 12614, 1073, 13883, 5272, 263, 32168, 687, 118, 2727, 1192, 23174, 28721, 255, 9662, 4, 1608, 21691, 19716, 3840, 3304, 1906, 1020, 2764, 5494, 853, 2095, 263, 5673, 42180, 3407, 16738, 4, 2271, 2195, 102, 842, 14156, 4349, 1615, 3304, 1906, 1020, 2764, 1832, 3807, 1371, 263, 24211, 1192, 117, 14701, 990, 763, 11330, 8647, 293, 6, 542, 6217, 1334, 263, 381, 530, 534, 10272, 11330, 8647, 293, 1192, 95, 1594, 9071, 225, 579, 3976, 9877, 36, 8105, 2764, 7619, 16312, 16446, 5251, 4261, 117, 3894, 8629, 102, 263, 2929, 1423, 475, 1526, 1178, 4261, 2269, 8629, 102, 263, 8757, 6, 2764, 132, 30109, 1977, 620, 7003, 13228, 4063, 922, 5347, 1423, 2908, 30109, 1977, 620, 7003, 46382, 9399, 4063, 922, 5347, 117, 25, 17641, 6510, 10, 515, 139, 3741, 17010, 2684, 43, 1423, 542, 29541, 4086, 506, 31263, 13883, 5272, 10272, 13516, 366, 263, 1760, 6837, 625, 4, 7925, 5285, 2628, 21691, 1535, 139, 117, 2489, 1455, 2102, 3741, 17010, 2426, 263, 475, 1322, 139, 1021, 579, 1977, 11326, 18836, 1423, 2489, 3137, 16481, 2102, 10, 740, 13941, 271, 10272, 10, 5489, 13850, 2727, 263, 579, 1977, 3999, 27745, 4, 1608, 2764, 3998, 5781, 242, 3137, 139, 6605, 1479, 12782, 354, 263, 3774, 579, 3976, 18836, 1615, 2664, 415, 424, 4843, 139, 2764, 5673, 42180, 3407, 16738, 2953, 4600, 1192, 842, 13085, 225, 4, 2588, 8556, 293, 17373, 5285, 1615, 21691, 1535, 139, 1368, 260, 1021, 3866, 718, 2102, 2694, 2794, 579, 661, 1479, 5895, 281, 263, 10529, 1021, 579, 661, 1479, 5895, 281, 263, 18918, 6, 2953, 4600, 1192, 842, 10, 6303, 1829, 1076, 2664, 415, 424, 4843, 139, 39420, 20363, 10, 741, 1176, 281, 385, 13310, 36, 225, 337, 1115, 20447, 195, 17844, 1076, 385, 5272, 322, 6007, 3741, 17010, 2684, 1423, 263, 7281, 118, 2727, 39929, 2617, 2953, 2628, 475, 1140, 417, 2684, 263, 9156, 8954, 4, 7879, 1021, 9738, 233, 242, 6, 842, 7630, 12145, 2520, 1479, 1177, 11540, 4138, 2520, 281, 542, 31031, 139, 263, 50, 1243, 10512, 1168, 1479, 571, 2684, 2764, 1717, 18375, 6070, 9697, 618, 9697, 3840, 381, 4, 37967, 118, 6, 2664, 415, 1526, 1187, 3876, 897, 4047, 242, 12870, 2727, 263, 50, 1243, 2764, 740, 1588, 1001, 4825, 4325, 1043, 1696, 11651, 4, 50118, 10766, 18234, 4843, 242, 12369, 2552, 542, 102, 494, 102, 263, 16057, 1180, 2520, 2727, 2953, 2628, 4803, 625, 1192, 9371, 338, 5272, 10, 18816, 271, 1615, 910, 918, 2977, 263, 6056, 1977, 417, 281, 2953, 4600, 1192, 842, 3872, 18832, 15205, 6674, 102, 2764, 25753, 2727, 6, 1717, 8, 9618, 6, 7630, 14369, 2003, 241, 5079, 856, 1977, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 256}"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["encoded_dataset = dataset.map(\n","    preprocess_data, batched=True, remove_columns=dataset['train'].column_names)\n","encoded_dataset.set_format(\"torch\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0y0vnLXxyOjs","executionInfo":{"status":"ok","timestamp":1679507270680,"user_tz":0,"elapsed":3339,"user":{"displayName":"Alexis Domínguez","userId":"12545493388305103028"}},"outputId":"f1694c84-95f7-48c9-8f20-641ae681f201"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:datasets.arrow_dataset:Loading cached processed dataset at /drive/My Drive/CorpusPFG/Dataset/train/cache-f7a7a9ba5041f4f4.arrow\n","WARNING:datasets.arrow_dataset:Loading cached processed dataset at /drive/My Drive/CorpusPFG/Dataset/validation/cache-e6617e3cde716941.arrow\n","WARNING:datasets.arrow_dataset:Loading cached processed dataset at /drive/My Drive/CorpusPFG/Dataset/test/cache-4c6c76913f732f65.arrow\n"]}]},{"cell_type":"code","source":["encoded_dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NsCXrJhDIVc7","executionInfo":{"status":"ok","timestamp":1679507270680,"user_tz":0,"elapsed":10,"user":{"displayName":"Alexis Domínguez","userId":"12545493388305103028"}},"outputId":"4ba07a8a-97cf-4f6b-a15f-f4c6b8039cfd"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['input_ids', 'attention_mask', 'labels'],\n","        num_rows: 2088\n","    })\n","    validation: Dataset({\n","        features: ['input_ids', 'attention_mask', 'labels'],\n","        num_rows: 233\n","    })\n","    test: Dataset({\n","        features: ['input_ids', 'attention_mask', 'labels'],\n","        num_rows: 583\n","    })\n","})"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["example = encoded_dataset['train'][0]\n","tokenizer.decode(example['input_ids'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":161},"id":"_LfdUdPdI8N7","executionInfo":{"status":"ok","timestamp":1679507270681,"user_tz":0,"elapsed":9,"user":{"displayName":"Alexis Domínguez","userId":"12545493388305103028"}},"outputId":"6ed89734-6359-442c-d96f-ec0f5a086223"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<s> _CABECERA_XXXX_  \\nSexo :Mujer\\n\\tINFORME ALTA DE MEDICINA INTERNA\\n\\tNIF/ _DATA_XXXX_ \\n\\t _DATA_XXXX_ \\n\\t _DATA_XXXX_ \\n\\t _DATA_XXXX_ \\n\\t_NOMBRE_XXXX_  \\n\\tTeléfono:  _DATA_XXXX_ \\n\\tTeléfono Móvil:  _DATA_XXXX_ \\n\\tFecha ingreso:  _FECHA_XXXX_   _HORA_XXXX_ \\nFecha alta:        _FECHA_XXXX_ \\n\\t _DOC_XXXX_ \\n\\tTipo de Ingreso: Desde urgencias\\n\\tMotivo de Ingreso (GP) Síncope posible hipotensivo. Problema social\\nMotivo de Alta\\n _APELLIDO_XXXX_ \\nMujer de 88 años.\\nAntecedentes Personales\\nNo refiere alergias conocidas a medicamentos\\nHTA. Cardiopatía hipertensiva con HVI moderada valorada por cardiología en 2008. Estudio de isquemia MIBI-DIPI negativo.\\nDM-2 sin tratamiento farmacológico.\\nDL.\\nNeoplasia maligna de útero \\nCarcinoma basocelular.\\nHerpes zoster torácico en 2010.\\nLumbalgia crónica mecánica. Aplastamiento de L1.\\nSd depresivo.\\nDeterioro cognitivo leve. Valorada por Neurología.\\n*Ingreso en 2015 en MINT por Neumonía LSD e ITU. Presentó SCA durante el ingreso.\\n*Episodios de urgencias por episodios de desorientación.\\nANTECEDENTES QUIRÚRGICOS: Cataratas bilateral, 2 cesáreas, fisura anal hace 15 años. Absceso isquiorectal (2008), reintervenido en dos ocasiones por proceso fistuloso.\\nSITUACIÓN BASAL: Vive en domicilio sola, activa e independiente para las ABVD. Doble continente. Deterioro cognitivo leve. No disnea ni ortopnea. No edemas en MMII. Anda con bastón.\\nTRATAMIENTO HABITUAL: COROPRES 25 1-0-0. DONEPEZILO 10. METAMIZOL. OMEPRAZOL 20 1-0-0. SERTRALINA 5 1-0-0,5. TRAMADOL/PARACETAMOL.\\nEnfermedad Actual\\nMujer de 88 años que acude a urgencias tras ser encontrada por su hijo en el suelo a las 19.00. La paciente presentó síncope con amnesia  aunque afirma pródromos al síncope consistente en visión borrosa y sudoración y en relación con la adopción de la bipedestación. Múltiples caídas de  _FECHA_XXXX_  para aquí en el contexto de la bipedestación, estando en el mercado. Asegura que siempre se cae hacia atrás con TCE occipital. Nota que las piernas no le aguantan y claudican. No relajación de esfínteres ni mordedura de lengua. Escoriaciones en ambos codos, y hematomas en ambas rodillas, dolor costal y lumbar. No dolor torácico previo ni palpitaciones ni sd miccional, ni dolor abdominal ni diarrea.\\nExploración Física\\nUrgencias\\nTA: 157/66; FC: 52 lpm. 95% basal.\\nHematomas en brazo izquierdo y pierna izquierda antiguos y recientes. A la exploración de hombro izquierdo no existe patrón capsular ni evidencia de rotura. Dolor a la palpación, no dolor a la movilización pasiva del miembro. AP: MVC sin ruidos sobreañadidos. AC: Rítmica sin soplos. ABD: RHA+, dolor a la palpación difusa, no masas ni megalias, timpánica, no peritonismo. MMII: No edemas, pulsos pedios conservados. NRL: No focalidad neurológica, moviliza 4 miembros, no rigidez de nuca, pares craneales normales, funciones superiores conservadas.\\nExploraciones Complementarias\\n  _FECHA_XXXX_  ECG: P sinusal  51 lpm. Rítmico. Eje normal. PR normal. QRS estrecho. QTc. No bloqueos ni datos de crecimiento de cavidades. Sin alteraciones agudas de la repolarización.\\n  _FECHA_XXXX_  Rx Tórax: No se observan claros infiltrados ni consolidaciones. No derrame pleural. ICT en límites de la normalidad.\\n  _FECHA_XXXX_  TC  craneal: No  se  observan  signos  de  sangrado  reciente,  colecciones  ni  efecto  de  masa  intra  ni  extraaxial. Correcta  diferenciación  entre  sustancia  blanca  y  sustancia  gris  supra  e  infratentorialmente,  apreciando  lesiones  hipodensas  en  sustancia  blanca  periventricular  en  probable  relación  con  enfermedad  isquémica  de  pequeño  vaso. Prominencia  de  surcos  y  ventrículos  en  relación  con  atrofia  corticosubcortical  difusa. Cisternas  libres.  Línea  media  centrada.  Ateromatosis  calcificada  de  sifones  carotídeos  y  arterias  vertebrales. Hiperostosis  frontal  interna \\nConclusión: \\nNo  se  observan  cambios  significativos  con  respecto  a  TC  previa  del  19/01/2015.\\n _FECHA_XXXX_  – Hemograma: LEU: 9.29 10^3/µL (3.50-11.00); Neut: 6.65 10^3/µL (2.0-7.5); linfoc: 1.77 10^3/µL (1.0-4.5); Monoci: 0.60 10^3/µL (0.2-0.8); Eosino: 0.00 10^3/µL (0.0-0.5); Basófi: 0.05 10^3/µL (0.0-0.2); LUC: 0.22 10^3µL (< =0); %Neut: 71.60 % (40.0-75.0); %Linfo: 19.10 % (20.0-45.0); %Monoc: 6.40 % (2.0-10.0); %Eosin: 0.00 % (1.0-6.0); %Basóf: 0.60 % (< =2); %LUC: 2.30 % (< =4); RBC: 4.59 10^6/µL (3.50-5.80); Hemogl: 14.30 g/dL (12.0-15.0); HTCO: 47.10 % (36.0-43.0)\\nVCM: 102.50 fL (78.0-100.0); HCM: 31.20 pg (27.0-32.0); CHCM: 30.50 g/dL (31.5-34.5); RDW-CV: 13.10 % (11.6-14.0); HDW: 2.78 g/dL (2.20-3.20); Plaquetas: 148.00 10^3/µL (130-450) Coagulación:  AP%: 107.40 % (80.0-120.0); INRa: 1.01 (< =1); APTT: 30.24 seg (25.0-35.0); Fi-der: 527.20 mg/dL (150.0-400.0); DDi: 1339.00 ng/ml (< =500) Bioquímica:  GLU: 82.00 mg/dl (70-110); URE: 76.00 mg/dl (10-50); CRE: 0.83 mg/dl (0.50-1.10); ALB: 3.20 g/dl (3.5-5.2); CA: 9.40 mg/dl (8.5-10.5); CAc: 10.3 mg/dl (8.6-10.2); NA: 139.00 mmol/L (135-147); K: 3.80 mmol/L (3.5-5.0); CL: 103.00 mmol/L (95-106); BT: 0.50 mg/dl (0.2-1.0); MIOG: 195.00 ng/ml (19.0-51.0); CK: 146.00 U/L (< =190); TNI: < 0.02 ng/ml (< =0); LDH: 288.00 U/L (80-235); GPT: 27.00 U/L (< =41); GOT: 28.00 U/L (< =31); FAL: 150.00 U/L (35-104) AMI: 29.00 U/L (< =100)\\n _FECHA_XXXX_  MN  Perfusión  pulmonar : Se  ha  realizado  gammagrafía  pulmonar,  despues  de  la  administración  por  vía  intravenosa  del  radiotrazador,  obteniendo  imágenes  planares,  en  proyecciones  anterior,  posterior,  laterales  y  oblicuas.  \\nPerfusión  pulmonar  homogénea  en  todos  los  campos,  sin  defectos  localizados  que  sugieran  TEP  en  la  actualidad. \\nResumen: Gammagrafía  pulmonar  sin  hallazgos  patológicos  significativos.  Se  descarta  razonablemente  la  existencia  de  TEP  en  la  actualidad. \\n11.09.2017Ecografía  Doppler  troncos  supraaórticos \\nTerritorio  carotídeo  DERECHO  de  características  morfológicas  normales.    No  hay  evidencia  de  ateromatosis  significativa.    Las  velocidades  y  curvas  espectrales  de  la  ACC,  ACI  y  ACE  están  dentro  de  límites  normales.    Se  detecta  flujo  en  la  arteria  vertebral  en  sentido  fisiológico.  Ratio  ACI/ACC:    0.64 \\nTerritorio  carotídeo  IZQUIERDO  de  características  morfológicas  normales.    No  hay  evidencia  de  ateromatosis  significativa.    Las  velocidades  y  curvas  espectrales  de  la  ACC,  ACI  y  ACE  están  dentro  de  límites  normales.    Se  detecta  flujo  en  la  arteria  vertebral  en  sentido  fisiológico.  Ratio  ACI/ACC:  1.32 \\nCONCLUSIÓN:  Sin  hallazgos  patológicos.\\n11/09/17-RX HOMBRO AP Y AXIAL: Sin alteraciones en marco óseo.\\n11/09/17Holter ECG: Ritmo sinusal con Fc media de 70, mínima de 48 y máxima de 98 lpm. 2 Extrasistoles ventriculares de 2 morfologías. 37 Extrasistoles supraventriculares. Asintomática durante el registro electrocardiográfico.\\n14-09-17-ESTUDIO EEG DE VIGILIA: Actividad de fondo simétrica, con ritmo alfa parieto occipital a 8 Hz. Ritmos rápidos de distribución fronto rolándica. \\nInterconsultas\\nAsuntos Sociales\\nMujer de 88 años de edad, viuda hace 20 años.Vive sola.\\nHijo único  _LOCALIDAD_XXXX_  móvil de contacto  _DATA_XXXX_  y nuera  _DATA_XXXX_.\\nTramitada solicitud de reconocimiento de la ley de dependencia desestimada en  _FECHA_XXXX_  de 2014.Servicios sociales El  _NOMBRE_XXXX_.\\nCoordinación para nueva cita e iniciar tramites de nuevo cita  _FECHA_XXXX_   a las  _HORA_XXXX_  horas.Entrego solicitud.\\nEntrego listado de empresas de asesoramiento de recursos y servicio de ayuda a domicilio.   \\nComentarios\\nMujer de 88 años que acude por episodio de pérdida de conciencia con pródromos previo, no observada por terceras personas, en paciente con betabloqueo como tratamiento antihipertensivo.  No datos indirectos de crisis tónico clónica (no pérdida control esfínteres o mordedura de lengua). No refiere disnea, palpitaciones o dolor torácico previo. Estos episodios han ocurrido en varias ocasiones previamente al episodio actual aunque no había consultado por ello. En urgencias no se objetivan taquicardias ni hipotensión, el EKG muestra un ritmo sinusal a 51 lpm, sin alteraciones del PR, QRS ni intervalo QT. Tampoco alteraciones destacables en placa de torax. No existen soplos sugerentes de estenosis aórtica severa y tiene un ecocardio previo sin valvulopatías. El TAC craneal no revela alteraciones significativas. Los marcadores enzimáticos cardíacos son normales. Gammagrafía de perfusión que descarta TEP. Se ingresa para estudio con retirada de betabloqueantes. En planta se completa el estudio con Doppler de TSA que no muestra alteraciones, un Holter de EKG sin alteraciones que justifiquen sincope (RS con frecuencia minima nocturna de 48 y máxima diurna de 98, con 2 extrasístoles ventriculares y 37 extrasístoles supraventriculares no asociados a evento clínico) y un electroencefalografía sin datos de actividad. Durante su ingreso no ha presentado clínica de mareo o síncopes y ha comenzado a caminar sin aparición de síntomas. Se concluye como hipótesis de los sincopes el tratamiento con betabloqueantes por lo que se suspenden. Las tensiones durante el ingreso han oscilado desde sistólicas de 125 o sistólicas de 155, por lo que se añade al tratamiento iecas a bajas dosis (enalapril 5 mg al día). Control clínico y de tensión arterial por su médico de primaria. Por otra parte, se evidenció en urgencias un sedimento de orina patológico con urocultivo postivo para E.Colli, tratándose la infección de orina con ciprofloxacino oral.\\nLa paciente tiene una Marcha de precaución por su edad que podría aumentar el riesgo de caídas por lo que se recomienda marcha con bastón, u andador, evitar barreras fí</s>'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["example['labels']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OLNwJnAlIjwT","executionInfo":{"status":"ok","timestamp":1679507270681,"user_tz":0,"elapsed":9,"user":{"displayName":"Alexis Domínguez","userId":"12545493388305103028"}},"outputId":"d1ba4465-8605-4b1a-9fa7-504d941c9914"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(256)"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["class2label.int2str(example['labels'].item())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"OmlJ_JoaJJSl","executionInfo":{"status":"ok","timestamp":1679507270681,"user_tz":0,"elapsed":7,"user":{"displayName":"Alexis Domínguez","userId":"12545493388305103028"}},"outputId":"19e904b9-c5f3-4c45-d256-241e2a83354c"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'T44.7X5A'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["## Train the model!\n","\n","We are going to train the model using HuggingFace's Trainer API."],"metadata":{"id":"5ODEcFBkJgAN"}},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","train_dataloader = DataLoader(\n","    encoded_dataset[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n",")\n","eval_dataloader = DataLoader(\n","    encoded_dataset[\"validation\"], batch_size=8, collate_fn=data_collator\n",")\n","\n","for batch in train_dataloader:\n","    break\n","{k: v.shape for k, v in batch.items()}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RQU7TCbP9oq9","executionInfo":{"status":"ok","timestamp":1679507270681,"user_tz":0,"elapsed":7,"user":{"displayName":"Alexis Domínguez","userId":"12545493388305103028"}},"outputId":"7ca64f19-b28f-410b-ee9a-59a8dceeed76"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["You're using a LongformerTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"output_type":"execute_result","data":{"text/plain":["{'input_ids': torch.Size([8, 4096]),\n"," 'attention_mask': torch.Size([8, 4096]),\n"," 'labels': torch.Size([8])}"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["from tqdm.auto import tqdm\n","from accelerate import Accelerator\n","from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler\n","import torch\n","from transformers import get_scheduler\n","\n","\n","#metrics\n","metric = load_metric(\"accuracy\")\n","\n","#accuracy compute\n","def compute_metrics(eval_pred):\n","  logits, labels = eval_pred\n","  predictions = np.argmax(logits, axis=-1)\n","  return metric.compute(predictions=predictions, references=labels)\n","\n","#def training_function():\n","\n","accelerator = Accelerator()\n","\n","model = AutoModelForSequenceClassification.from_pretrained(\n","      CHECKPOINT, \n","      num_labels=class2label.num_classes, \n","      id2label = id2label, \n","      label2id = label2id,\n","      problem_type = \"single_label_classification\") #\"multi_label_classification\"\n","\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","model.to(device)\n","\n","optimizer = AdamW(model.parameters(), lr=3e-5)\n","train_dl, eval_dl, model, optimizer = accelerator.prepare(\n","    train_dataloader, eval_dataloader, model, optimizer\n",")\n","\n","num_training_steps = NUM_EPOCHS * len(train_dataloader)\n","lr_scheduler = get_scheduler(\n","    \"linear\",\n","    optimizer=optimizer,\n","    num_warmup_steps=0,\n","    num_training_steps=num_training_steps,\n",")\n","\n","progress_bar = tqdm(range(num_training_steps))\n","\n","training_args = TrainingArguments(\n","    #evaluation_strategy=\"epoch\", \n","    #save_strategy = \"epoch\",\n","    #load_best_model_at_end=True,\n","    output_dir= MODEL_OUTPUT_DIR, \n","    per_device_train_batch_size=BATCH_SIZE,\n","    per_device_eval_batch_size=BATCH_SIZE,\n","    num_train_epochs=NUM_EPOCHS,\n","    learning_rate=2e-5,\n","    weight_decay=0.01,\n","    metric_for_best_model=METRIC_NAME,\n","    hub_token=HUGGING_FACE_TOKEN,\n","    hub_private_repo=True,\n","    push_to_hub=True) \n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=encoded_dataset[\"train\"],\n","    eval_dataset=encoded_dataset[\"validation\"],\n","    compute_metrics=compute_metrics,\n","    optimizers=(optimizer,lr_scheduler)\n",")\n","\n","trainer.train()\n","\n","tokenizer.push_to_hub(\n","    MODEL_OUTPUT_DIR, \n","    private=True, \n","    use_auth_token=HUGGING_FACE_TOKEN)\n","\n","  #model.train()\n","\n","  #for epoch in range(NUM_EPOCHS):\n","  #   for batch in train_dl:\n","  #       outputs = model(**batch)\n","  #       loss = outputs.loss\n","  #       accelerator.backward(loss)\n","\n","  #       optimizer.step()\n","  #       lr_scheduler.step()\n","  #       optimizer.zero_grad()\n","  #       progress_bar.update(1)    "],"metadata":{"id":"EKJXUS7Z_7eQ","executionInfo":{"status":"error","timestamp":1679507361569,"user_tz":0,"elapsed":90894,"user":{"displayName":"Alexis Domínguez","userId":"12545493388305103028"}},"colab":{"base_uri":"https://localhost:8080/","height":780,"referenced_widgets":["d09d002581cf479e976907f9ff7d8205","fa0b993c3f28411fa482ae1ad932b1f3","cc8ec622085448d682d93c6834b29785","3115a628b95c48d58cabf6718ae66c5d","8adb63a6c852441d9e48cd3ab6c9b154","f83d76332898468db2ff7c5f54609c61","2df7162dd48443d4b8375a0b7bf1ad1e","e622836f77a04b3cad68768b363e9ccd","2a806d80770e48079e99927953aa2725","1af82d14b9bc452991d46f443911fa47","3e8bea9e21984858bd906126ede010b3","3a270831382f4f288b99fb408068570c","b412b901863b45feb83bb8838c78eeec","bec33c01091541b8b6a995e5e798fee1","61bad095219d4b43bf523044ef3d881c","1f878c3ec7534f58a4e2939de16a3394","a1e2514e061e43389a71797b3bfd5979","40ade0a89cf34d85a48deaeb7479aacf","0fd437c2d4b94a5f82eb5ffc2e3b53ed","79b54843a9644940b4c84b7ab3330325","d8112dd59deb4569bd6357975886f488","1a5533a7b2614321b9cd380ba8b5855f","d850d82a9b494c1596e9a39b49a539c3","96408953079b4b12ad2ce2641fe26327","1e2ee61c1165475ebe4a1506eb0d4672","3785385707704df99b34da0e82a8ff2a","349c223e7a4d4222971f1f7d610f97d5","4e5251da32c44018ae40b706b6df63e9","8b68421853494acdb31d3f48d741a991","72dbcea5dfcb425f9e9e15c4c667f9b7","a7f729c185f149be95ae2eeea6cf2362","9cb07cac73184dc49a461a6b5cbe20d1","8bf7af91b3344e6da4cae2031d590263"]},"outputId":"23e091a6-c5ee-46c4-f613-11137252dcca"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-19-d81f98a2d425>:9: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n","  metric = load_metric(\"accuracy\")\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading builder script:   0%|          | 0.00/1.65k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d09d002581cf479e976907f9ff7d8205"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/597M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a270831382f4f288b99fb408068570c"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1305 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d850d82a9b494c1596e9a39b49a539c3"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Cloning https://huggingface.co/alexisdr/uned-tfg-07.02 into local empty directory.\n","WARNING:huggingface_hub.repository:Cloning https://huggingface.co/alexisdr/uned-tfg-07.02 into local empty directory.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='46' max='2610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [  46/2610 01:10 < 1:08:33, 0.62 it/s, Epoch 0.09/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-d81f98a2d425>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     69\u001b[0m )\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m tokenizer.push_to_hub(\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m         )\n\u001b[0;32m-> 1633\u001b[0;31m         return inner_training_loop(\n\u001b[0m\u001b[1;32m   1634\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1635\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1900\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1901\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1902\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1904\u001b[0m                 if (\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2644\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2645\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2647\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2675\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2676\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2677\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2678\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2679\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, global_attention_mask, head_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1936\u001b[0m             \u001b[0mglobal_attention_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1938\u001b[0;31m         outputs = self.longformer(\n\u001b[0m\u001b[1;32m   1939\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1940\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, global_attention_mask, head_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1748\u001b[0m         )\n\u001b[1;32m   1749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1750\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1751\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, padding_len, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1324\u001b[0m                 )\n\u001b[1;32m   1325\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1326\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m   1327\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[1;32m   1247\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m     ):\n\u001b[0;32m-> 1249\u001b[0;31m         self_attn_outputs = self.attention(\n\u001b[0m\u001b[1;32m   1250\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[1;32m   1183\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m     ):\n\u001b[0;32m-> 1185\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m   1186\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[1;32m    584\u001b[0m         )\n\u001b[1;32m    585\u001b[0m         \u001b[0;31m# diagonal mask with zeros everywhere and -inf inplace of padding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m         diagonal_mask = self._sliding_chunks_query_key_matmul(\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0mfloat_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_ones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_sided_attn_window_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36m_sliding_chunks_query_key_matmul\u001b[0;34m(self, query, key, window_overlap)\u001b[0m\n\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         diagonal_attention_scores = diagonal_chunked_attention_scores.new_zeros(\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks_count\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_overlap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_overlap\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         )\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["#from accelerate import notebook_launcher\n","\n","#notebook_launcher(training_function)"],"metadata":{"id":"gCKnrKgTKgbt","executionInfo":{"status":"aborted","timestamp":1679507361570,"user_tz":0,"elapsed":5,"user":{"displayName":"Alexis Domínguez","userId":"12545493388305103028"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Evaluate\n","\n","After training, we evaluate our model on the validation set."],"metadata":{"id":"e3lmC1ymKSLT"}},{"cell_type":"code","source":["#trainer.evaluate()"],"metadata":{"id":"OFmahBbacumN","executionInfo":{"status":"aborted","timestamp":1679507361570,"user_tz":0,"elapsed":4,"user":{"displayName":"Alexis Domínguez","userId":"12545493388305103028"}}},"execution_count":null,"outputs":[]}]}